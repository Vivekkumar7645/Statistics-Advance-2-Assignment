{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Statistics Advance 2 Assignment"
      ],
      "metadata": {
        "id": "BXc1zoe-7Bzu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q1. What is hypothesis testing in statistics?\n",
        "**Ans** - Hypothesis testing in statistics is a method used to make decisions or inferences about a population based on sample data. It involves the following steps:\n",
        "1. Formulating Hypotheses:\n",
        "  * Null Hypothesis: This is a statement that there is no effect or no difference, and it represents the status quo or a baseline assumption.\n",
        "  * Alternative Hypothesis: This is a statement that there is an effect, a difference, or a relationship that contradicts the null hypothesis.\n",
        "2. Choosing a Significance Level:\n",
        "  * The significance level, often denoted as α, is the probability of rejecting the null hypothesis when it is actually true. Common values for α are 0.05, 0.01, or 0.10.\n",
        "3. Selecting a Test Statistic:\n",
        "  * Depending on the type of data and the hypotheses, a specific statistical test is chosen.\n",
        "4. Calculating the Test Statistic and P-value:\n",
        "  * The test statistic is computed from the sample data, and the p-value is determined, which indicates the probability of observing the test results under the null hypothesis.\n",
        "5. Making a Decision:\n",
        "  * If the p-value is less than or equal to the significance level, the null hypothesis is rejected in favor of the alternative hypothesis.\n",
        "  * If the p-value is greater than the significance level, there is insufficient evidence to reject the null hypothesis."
      ],
      "metadata": {
        "id": "ETu6q5Ay7LHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q2. What is the null hypothesis, and how does it differ from the alternative hypothesis?\n",
        "**Ans**- The null hypothesis and the alternative hypothesis are fundamental concepts in hypothesis testing, serving as the basis for making statistical inferences:\n",
        "\n",
        "**Null Hypothesis:**\n",
        "* The null hypothesis is a statement that there is no effect, no difference, or no relationship in the population. It represents the default or baseline assumption that any observed effect in the sample data is due to chance.\n",
        "* Purpose: It serves as the statement to be tested, and the goal of hypothesis testing is to determine whether there is enough evidence to reject the null hypothesis.\n",
        "* Example: If testing whether a new drug is effective, the null hypothesis might state that the drug has no effect on patients.\n",
        "\n",
        "**Alternative Hypothesis**:\n",
        "* The alternative hypothesis is a statement that suggests there is an effect, a difference, or a relationship in the population. It represents what the researcher aims to support or prove.\n",
        "* Purpose: It provides a contrasting statement to the null hypothesis and is considered if the null hypothesis is rejected.\n",
        "* Example: For the same drug study, the alternative hypothesis might state that the drug does have an effect on patients.\n",
        "\n",
        "**Differences**:\n",
        "1. Purpose:\n",
        "  * H₀: Assumes no effect or difference.\n",
        "  * H₁: Suggests there is an effect or difference.\n",
        "2. Testing:\n",
        "  * H₀: Tested directly to see if it can be rejected.\n",
        "  * H₁: Accepted if there is enough evidence to reject H₀.\n",
        "3. Outcomes:\n",
        "* Rejecting H₀ suggests that the data supports H₁.\n",
        "* Failing to reject H₀ means there is not enough evidence to support H₁, but it does not prove H₀ is true."
      ],
      "metadata": {
        "id": "RaI1vqOj7gt4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q3. What is the significance level in hypothesis testing, and why is it important?\n",
        "**Ans** - The significance level in hypothesis testing is a threshold that determines how strong the evidence must be before rejecting the null hypothesis. It is a key component in the decision-making process and represents the probability of making a Type I error—rejecting the null hypothesis when it is actually true.\n",
        "\n",
        "**Importance of the Significance Level**:\n",
        "1. Controls Type I Error:\n",
        "  * The significance level sets the risk of falsely rejecting a true null hypothesis. For example, a significance level of 0.05 implies a 5% risk of concluding that there is an effect when there is none.\n",
        "2. Decision Threshold:\n",
        "  * The significance level defines the cutoff for determining whether the p-value is small enough to reject the null hypothesis. If the p-value is less than or equal to α, the null hypothesis is rejected.\n",
        "3. Consistency in Testing:\n",
        "  * Establishing a significance level before conducting a test ensures consistency and objectivity in hypothesis testing. It prevents researchers from making ad hoc decisions based on the data alone.\n",
        "4. Interpreting Results:\n",
        "  * The chosen significance level helps interpret the results of the test. For example:\n",
        "    * If α = 0.05 and the p-value is 0.03, the result is statistically significant, and the null hypothesis is rejected.\n",
        "    * If the p-value is 0.07, the result is not statistically significant at the 0.05 level, and the null hypothesis is not rejected.\n",
        "\n",
        "**Common Significance Levels**:\n",
        "* 0.05 (5%): Widely used in many fields; it strikes a balance between being too lenient and too strict.\n",
        "* 0.01 (1%): Used when stronger evidence is required, often in fields like medicine or hard sciences.\n",
        "* 0.10 (10%): Sometimes used in exploratory studies where a higher tolerance for Type I error is acceptable."
      ],
      "metadata": {
        "id": "EF4llH7B7qJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q4. What does a P-value represent in hypothesis testing?\n",
        "**Ans** - In hypothesis testing, the p-value represents the probability of observing the test statistic, or something more extreme, under the assumption that the null hypothesis is true. It quantifies the strength of the evidence against the null hypothesis.\n",
        "\n",
        "**Aspects of the P-value**:\n",
        "1. Probability Measure:\n",
        "  * The p-value is a probability that measures the compatibility of the observed data with the null hypothesis.\n",
        "  * A low p-value indicates that the observed data is unlikely under the null hypothesis, suggesting stronger evidence against H₀.\n",
        "2. Decision Making:\n",
        "  * The p-value is compared to the significance level to make a decision:\n",
        "    * p ≤ α: Reject the null hypothesis.\n",
        "    * p > α: Fail to reject the null hypothesis.\n",
        "3. Interpretation:\n",
        "  * Small p-value (typically ≤ 0.05): Strong evidence against the null hypothesis, leading to its rejection.\n",
        "  * Large p-value (typically > 0.05): Weak evidence against the null hypothesis, so it is not rejected.\n",
        "  * P-value close to α: The result is on the borderline, making the decision less clear-cut.\n",
        "4. Context-Dependent:\n",
        "  * The p-value alone does not measure the magnitude of the effect or the importance of the result. It must be interpreted in the context of the study, alongside other statistics like effect size and confidence intervals.\n",
        "\n",
        "**Example**:\n",
        "* Suppose we are testing whether a new drug is more effective than a placebo.\n",
        "  * Null Hypothesis: The drug has no effect (mean difference = 0).\n",
        "  * Alternative Hypothesis: The drug has an effect (mean difference ≠ 0).\n",
        "  * After conducting the test, we calculate a p-value of 0.02.\n",
        "\n",
        "Since the p-value (0.02) is less than the typical significance level (0.05), we reject the null hypothesis, concluding that there is evidence to suggest the drug is effective."
      ],
      "metadata": {
        "id": "j5IA0ecC70RP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q5. How do you interpret the P-value in hypothesis testing?\n",
        "**Ans** - Interpreting the p-value in hypothesis testing involves understanding what it indicates about the relationship between the observed data and the null hypothesis. Steps for interpret it:\n",
        "\n",
        "1. **P-value and the Null Hypothesis**:\n",
        "* The p-value is the probability of observing the test statistic, or one more extreme, assuming the null hypothesis is true.\n",
        "* It helps determine whether the observed data is consistent with the null hypothesis.\n",
        "2. **Decision Rule**:\n",
        "* Compare the p-value to the significance level, typically set at 0.05, 0.01, or 0.10.\n",
        "* If p ≤ α: Reject the null hypothesis. There is sufficient evidence to support the alternative hypothesis.\n",
        "* If p > α: Fail to reject the null hypothesis. There is not enough evidence to support the alternative hypothesis.\n",
        "3. **Strength of Evidence**:\n",
        "* Small p-value (≤ 0.05): Strong evidence against the null hypothesis. The data is unlikely to have occurred under the null hypothesis.\n",
        "* Moderate p-value: Some evidence against the null hypothesis but not strong enough to reject it at common significance levels.\n",
        "* Large p-value (> 0.05): Weak or no evidence against the null hypothesis. The data is consistent with H₀.\n",
        "4. **P-value Does Not:**\n",
        "* Indicate the truth: A small p-value does not prove the alternative hypothesis is true, nor does a large p-value prove the null hypothesis is true.\n",
        "* Measure effect size: The p-value does not tell us how large or important the observed effect is.\n",
        "* Provide the probability of H₀ being true: It does not directly indicate the probability that the null hypothesis is true or false.\n",
        "5. **Example Interpretation**:\n",
        "* Suppose we test a new teaching method and obtain a p-value of 0.03.\n",
        "  * If our significance level is 0.05, we reject the null hypothesis, concluding there is evidence that the teaching method has an effect.\n",
        "  * If our significance level is 0.01, we fail to reject the null hypothesis, as the evidence is not strong enough at this more stringent level."
      ],
      "metadata": {
        "id": "GBYrkEQg78lW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q7. What are Type 1 and Type 2 errors in hypothesis testing?\n",
        "**Ans** - In hypothesis testing, Type I and Type II errors are potential mistakes that can occur when making decisions about the null hypothesis. Understanding these errors is crucial for interpreting the results of a hypothesis test.\n",
        "\n",
        "**Type I Error (False Positive)**:\n",
        "* Type I error occurs when the null hypothesis is rejected, even though it is actually true.\n",
        "* Significance Level: The probability of making a Type I error is represented by the significance level. For example, if α = 0.05, there is a 5% chance of incorrectly rejecting the null hypothesis.\n",
        "* Consequence: This error leads to the conclusion that there is an effect or difference when, in fact, there is none.\n",
        "* Example: In a clinical trial, concluding that a new drug is effective when it actually has no effect.\n",
        "\n",
        "**Type II Error (False Negative)**:\n",
        "* Type II error occurs when the null hypothesis is not rejected, even though the alternative hypothesis is true.\n",
        "* Probability: The probability of making a Type II error is denoted by β. The power of a test, which is 1 - β, represents the probability of correctly rejecting the null hypothesis when the alternative hypothesis is true.\n",
        "* Consequence: This error leads to the conclusion that there is no effect or difference when there actually is one.\n",
        "* Example: In a clinical trial, concluding that a new drug is not effective when it actually is effective.\n",
        "\n",
        "**Comparison of Type I and Type II Errors**:\n",
        "\n",
        "|Error Type\t|Definition\t|Consequence\t|Probability Notation|\n",
        "|---|---|---|---|\n",
        "|Type I\t|Rejecting H₀ when H₀ is true\t|False positive, incorrect rejection\t|α|\n",
        "|Type II\t|Failing to reject H₀ when H₁ is true\t|False negative, incorrect acceptance\t|β|"
      ],
      "metadata": {
        "id": "FCOJCSDB8FCT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q8. What is the difference between a one-tailed and a two-tailed test in hypothesis testing?\n",
        "**Ans** - In hypothesis testing, the choice between a one-tailed and a two-tailed test depends on the research question and the direction of the hypothesis.\n",
        "\n",
        "**One-Tailed Test**:\n",
        "* A one-tailed test assesses whether the sample mean is significantly greater than or less than a specific value, focusing on only one direction of the effect.\n",
        "* Hypotheses:\n",
        "  * Null Hypothesis: The parameter is equal to a specific value.\n",
        "  * Alternative Hypothesis: The parameter is either greater than or less than that specific value.\n",
        "* Example:\n",
        "  * Right-tailed: H₀: μ ≤ μ₀ vs. H₁: μ > μ₀ (testing if the mean is greater than μ₀).\n",
        "  * Left-tailed: H₀: μ ≥ μ₀ vs. H₁: μ < μ₀ (testing if the mean is less than μ₀).\n",
        "* Use: When the research question or theory predicts a specific direction of the effect.\n",
        "* Significance Level: Entire α is applied to one tail of the distribution.\n",
        "\n",
        "**Two-Tailed Test**:\n",
        "* A two-tailed test evaluates whether the sample mean is significantly different from a specific value, considering both directions.\n",
        "* Hypotheses:\n",
        "  * Null Hypothesis: The parameter is equal to a specific value.\n",
        "  * Alternative Hypothesis: The parameter is not equal to that value.\n",
        "* Example:\n",
        "  * H₀: μ = μ₀ vs. H₁: μ ≠ μ₀ (testing if the mean is either greater than or less than μ₀).\n",
        "* Use: When the research question does not predict the direction of the effect or when both directions are of interest.\n",
        "* Significance Level: The α is split equally between both tails of the distribution.\n",
        "\n",
        "**Differences**:\n",
        "\n",
        "|Feature\t|One-Tailed Test\t|Two-Tailed Test|\n",
        "|---|---|---|\n",
        "|Hypothesis\t|Tests for an effect in one direction\t|Tests for an effect in both directions|\n",
        "|P-value\t|Calculated for one tail\t|Calculated for both tails|\n",
        "|Significance Level\t|Entire α in one tail\t|α/2 in each tail|\n",
        "|Use Cases\t|Directional hypotheses (greater or less)\t|Non-directional hypotheses (different)|"
      ],
      "metadata": {
        "id": "REkwG2-Y8MJi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q9. What is the Z-test, and when is it used in hypothesis testing?\n",
        "**Ans** - The Z-test is a statistical test used to determine whether there is a significant difference between the means of a sample and a population or between the means of two samples. It is based on the assumption that the data follows a normal distribution and the population variance is known or the sample size is large.\n",
        "\n",
        "**Use of Z-test**:\n",
        "1. Comparing a Sample Mean to a Population Mean:\n",
        "* When the population variance is known.\n",
        "* When the sample size is large, even if the population variance is unknown, because the Central Limit Theorem ensures the sampling distribution of the mean is approximately normal.\n",
        "2. Comparing Two Sample Means:\n",
        "* When comparing the means of two independent samples.\n",
        "* Both samples should be large, or the population variance should be known.\n",
        "3. Proportion Tests:\n",
        "* When testing hypotheses about population proportions."
      ],
      "metadata": {
        "id": "2VcNk7Z48WWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q10. How do you calculate the Z-score, and what does it represent in hypothesis testing?\n",
        "**Ans** - **Calculation the Z-score**:\n",
        "\n",
        "The Z-score is a measure of how many standard deviations an element is from the mean. In hypothesis testing, it helps determine how unusual or typical a data point is under the null hypothesis.\n",
        "\n",
        "**Z-score Represents in Hypothesis Testing**:\n",
        "1. Standardization:\n",
        "  * The Z-score standardizes the test statistic by converting it into a standard normal distribution.\n",
        "2. Distance from Mean:\n",
        "  * The Z-score measures how many standard deviations the sample mean is from the population mean under the null hypothesis.\n",
        "3. Comparison to Critical Values:\n",
        "  * The Z-score is compared to critical values from the Z-distribution:\n",
        "    * For a two-tailed test with α = 0.05, the critical values are approximately ±1.96.\n",
        "    * For a one-tailed test with α = 0.05, the critical value is approximately 1.645 (right-tailed) or -1.645 (left-tailed).\n",
        "4. Decision Rule:\n",
        "  * If |Z| > critical value: Reject the null hypothesis.\n",
        "  * If |Z| ≤ critical value: Fail to reject the null hypothesis.\n",
        "5. P-value Interpretation:\n",
        "  * The Z-score can also be used to calculate the p-value, which indicates the probability of observing a Z-score as extreme as the one calculated, under the null hypothesis.\n",
        "  * A smaller p-value indicates stronger evidence against the null hypothesis.\n",
        "\n",
        "**Example**:\n",
        "* Suppose a researcher tests whether the average weight of a sample (mean = 75 kg, n = 30, σ = 10 kg) differs from the population mean of 70 kg.\n",
        "  * Z-score calculation:\n",
        "        Z = (75-70)/{10/30^(1/2)} ≈ 2.74\n",
        "  * This Z-score is compared to the critical value (1.96 for α = 0.05 in a two-tailed test). Since 2.74 > 1.96, the null hypothesis is rejected."
      ],
      "metadata": {
        "id": "t2bMjjwg8ff3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q11. What is the T-distribution, and when should it be used instead of the normal distribution?\n",
        "**Ans** - The T-distribution, also known as the Student's t-distribution, is a probability distribution that is similar in shape to the normal distribution but has heavier tails. It is used in hypothesis testing and confidence interval estimation, particularly when dealing with small sample sizes or when the population standard deviation is unknown.\n",
        "\n",
        "**Characteristics of the T-distribution**:\n",
        "1. Shape:\n",
        "  * Similar to the normal distribution but with thicker tails, meaning it has a higher probability of values far from the mean.\n",
        "  * As the sample size increases, the T-distribution approaches the normal distribution.\n",
        "2. Degrees of Freedom:\n",
        "  * The shape of the T-distribution depends on the degrees of freedom, which are typically related to the sample size.\n",
        "  * Fewer degrees of freedom result in a distribution with heavier tails.\n",
        "3. Mean:\n",
        "  * The mean of the T-distribution is 0, similar to the normal distribution.\n",
        "\n",
        "**Use of the T-distribution**:\n",
        "1. Small Sample Size (n < 30):\n",
        "  * When the sample size is small, the sample mean's distribution is better approximated by the T-distribution rather than the normal distribution.\n",
        "2. Unknown Population Standard Deviation:\n",
        "  * If the population standard deviation is unknown and needs to be estimated from the sample standard deviation, the T-distribution should be used.\n",
        "3. Hypothesis Testing and Confidence Intervals:\n",
        "  * In situations where the sample size is small and the population standard deviation is unknown, such as in t-tests.\n",
        "\n",
        "**Comparison with the Normal Distribution**:\n",
        "\n",
        "|Feature\t|T-distribution\t|Normal Distribution|\n",
        "|----|----|----|\n",
        "|Shape\t|Bell-shaped with heavier tails\t|Bell-shaped with thinner tails|\n",
        "|Use Case\t|Small sample sizes, unknown σ\t|Large sample sizes, known σ|\n",
        "|Degrees of Freedom\t|Varies with sample size\t|Not dependent on sample size|\n",
        "|Approaches Normal\t|As n increases\t|Always the same|"
      ],
      "metadata": {
        "id": "kYT2uTag8qnP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 12. What is the difference between a Z-test and a T-test?\n",
        "**Ans** - The Z-test and the T-test are both statistical tests used to compare sample data to a population or compare two samples, but they differ in the conditions under which each is used and the assumptions they rely on. Here's a detailed comparison:\n",
        "1. **Population Standard Deviation Known vs. Unknown**:\n",
        "* Z-test:\n",
        "  * Requires the population standard deviation to be known or the sample size to be large enough (typically n≥30) so that the Central Limit Theorem applies, making the sampling distribution approximately normal even if the population standard deviation is unknown.\n",
        "* T-test:\n",
        "  * Does not require the population standard deviation. It is used when the population standard deviation is unknown and must be estimated from the sample.\n",
        "  * More appropriate for small sample sizes where the population variance is unknown.\n",
        "2. **Sample Size**:\n",
        "* Z-test:\n",
        "  * Typically used when the sample size is large (n≥30).\n",
        "  * When sample size is large, even if the population standard deviation is unknown, the sample standard deviation can serve as a good approximation.\n",
        "* T-test:\n",
        "  * Typically used for small sample sizes (less than 30) when the population standard deviation is unknown.\n",
        "  * The T-test adjusts for the increased uncertainty in smaller samples.\n",
        "3. **Test Statistic**:\n",
        "* Z-test:\n",
        "  * The Z-test uses the Z-statistic, calculated as:\n",
        "    \n",
        "        Z = (Xi-μ)/{σ/n^(1/2)}\n",
        "\n",
        "  Where:\n",
        "  * Xi = Sample mean\n",
        "  * μ = Population mean\n",
        "  * σ = Population standard deviation\n",
        "  * n = Sample size\n",
        "\n",
        "* T-test:\n",
        "  * The T-test uses the t-statistic, calculated as:\n",
        "\n",
        "        t = (Xi-μ)/{s/n^(1/2)}\n",
        "  Where:\n",
        "  * Xi = Sample mean\n",
        "  * μ = Population mean\n",
        "  * s = Sample standard deviation\n",
        "  * n = Sample size\n",
        "\n",
        "* The t-statistic accounts for the sample size by incorporating the sample standard deviation, and it follows a T-distribution instead of a normal distribution.\n",
        "4. **Shape of the Distribution**:\n",
        "* Z-test:\n",
        "  * The Z-test uses the normal distribution as its reference distribution.\n",
        "* T-test:\n",
        "  * The T-test uses the T-distribution, which has heavier tails than the normal distribution. The exact shape of the T-distribution depends on the degrees of freedom, which is related to the sample size (df=n-1).\n",
        "  * As the sample size increases, the T-distribution approaches the normal distribution.\n",
        "5. **Critical Values**:\n",
        "* Z-test:\n",
        "  * The Z-test uses the Z-table to find critical values or p-values based on the standard normal distribution.\n",
        "* T-test:\n",
        "  * The T-test uses the t-table to find critical values or p-values based on the T-distribution, which varies depending on the degrees of freedom.\n",
        "6. **Applications**:\n",
        "* Z-test:\n",
        "  * Used when:\n",
        "    * The sample size is large (n≥30) or the population standard deviation is known.\n",
        "    * Comparing a sample mean to a population mean or comparing two sample means when the standard deviation is known or the sample size is large.\n",
        "* T-test:\n",
        "  * Used when:\n",
        "    * The sample size is small (n < 30) and the population standard deviation is unknown.\n",
        "    * The population variance is estimated from the sample (s).\n",
        "    * Examples include one-sample t-tests, two-sample t-tests, and paired sample t-tests.\n",
        "7. **Example of Use Each**:\n",
        "* Z-test:\n",
        "  * Suppose a factory produces light bulbs, and the population standard deviation of the bulb's lifetime is known. If we want to test whether a sample of 100 bulbs has an average lifetime different from the population mean, we would use a Z-test.\n",
        "* T-test:\n",
        "  * Suppose we want to test whether a new teaching method improves test scores. we have a small sample of 15 students, and we don't know the population standard deviation. In this case, a T-test would be appropriate.\n",
        "\n",
        "**Table**:\n",
        "\n",
        "|Feature\t|Z-test\t|T-test|\n",
        "|-|||\n",
        "|Population Standard Deviation\t|Known or large sample size\t|Unknown, estimated from sample|\n",
        "|Sample Size\t|Large (n≥30)\t|Small (n < 30)|\n",
        "|Test Statistic\t|Z-statistic\t|t-statistic|\n",
        "|Distribution\t|Normal Distribution (Z-distribution)\t|T-distribution (depends on degrees of freedom)|\n",
        "|Application\t|Large samples, known σ\t|Small samples, unknown σ|"
      ],
      "metadata": {
        "id": "f9OCi8uv8w-U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 13. What is the T-test, and how is it used in hypothesis testing?\n",
        "\n",
        "**Ans** - The T-test is a statistical test used to determine whether there is a significant difference between the means of two groups or between a sample mean and a population mean. It is used when the sample size is small or when the population standard deviation is unknown, and the sample standard deviation is used instead. The T-test is based on the T-distribution, which adjusts for the added uncertainty in small samples.\n",
        "\n",
        "**Types of T-tests**:\n",
        "\n",
        "There are three main types of T-tests:\n",
        "1. One-Sample T-test:\n",
        "  * Compares the mean of a single sample to a known population mean.\n",
        "  * Used when we want to test if the sample mean is significantly different from a known value or population mean.\n",
        "  * Example: Testing whether the average height of a sample of students differs from the national average height.\n",
        "\n",
        "* Hypotheses:\n",
        "  * Null Hypothesis:μ = μ0 (the sample mean is equal to the population mean)\n",
        "  * Alternative Hypothesis:μ ≠ μ0 (the sample mean is different from the population mean)\n",
        "\n",
        "2. Two-Sample T-test:\n",
        "* Compares the means of two independent samples to see if they are significantly different.\n",
        "* Used when we want to test if two groups have different means, such as comparing the test scores of two different classrooms.\n",
        "* Example: Testing whether the average scores of two different teaching methods are different.\n",
        "\n",
        "* Hypotheses:\n",
        "  * Null Hypothesis:μ1 = μ2 (the means of the two groups are equal)\n",
        "  * Alternative Hypothesis:μ1 ≠ μ2 (the means of the two groups are different)\n",
        "\n",
        "3. Paired Sample T-test:\n",
        "* Compares the means of two related groups, where each participant is measured twice.\n",
        "* Example: Testing whether the average weight of participants decreases after a diet intervention.\n",
        "\n",
        "* Hypotheses:\n",
        "  * Null Hypothesis:μdifference = 0 (no difference between paired samples)\n",
        "  * Alternative Hypothesis:μdifference ≠ 0 (there is a difference between paired samples)\n",
        "\n",
        "**Use of T-test:**\n",
        "\n",
        "Scenario: A researcher wants to test whether the average test score of students in a class (n = 25) is significantly different from the population average test score of 75.\n",
        "* Step 1: Formulate Hypotheses:\n",
        "  * Null Hypothesis (H₀):μ = 75\n",
        "  * Alternative Hypothesis (H₁):μ ≠ 75\n",
        "* Step 2: Select the Significance Level:\n",
        "  * α = 0.05\n",
        "* Step 3: Calculate the T-statistic:\n",
        "  * Suppose the sample mean Xi = 78, sample standard deviation s = 10, and n = 25.\n",
        "  * Using the formula:\n",
        "\n",
        "         t = (78-75)/{10/25^(1/2)} = 3/2 = 1.5\n",
        "\n",
        "* Step 4: Find the Critical Value or p-value:\n",
        "  * For a two-tailed test with df = n-1 = 24 and α = 0.05, we look up the critical t-value from the T-distribution table. For df=24, the critical value is approximately 2.064.\n",
        "  * Since 1.5 < 2.064, we fail to reject the null hypothesis.\n",
        "* Step 5: Make a Decision:\n",
        "  * Since the calculated t-value (1.5) is less than the critical t-value (2.064), we fail to reject the null hypothesis. There is not enough evidence to conclude that the sample mean is significantly different from the population mean."
      ],
      "metadata": {
        "id": "FauPXCBW86Xv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 14. What is the relationship between the Z-test and T-test in hypothesis testing?\n",
        "**Ans** - The Z-test and T-test are both used for hypothesis testing, and both test the difference between sample statistics and population parameters. However, the choice between a Z-test and a T-test depends on the sample size, whether the population standard deviation is known, and the shape of the underlying data distribution.\n",
        "\n",
        "**Relationships Between the Z-test and T-test:**\n",
        "1. Both Test Hypotheses About Means:\n",
        "  * Both tests are used to assess hypotheses about the mean of a sample, often comparing the sample mean to a population mean or comparing means between two independent or paired samples.\n",
        "2. T-test is a Special Case of the Z-test:\n",
        "  * When the sample size is large (n≥30) or when the population standard deviation is known, the T-distribution approaches the normal distribution. In such cases, the T-test and the Z-test produce nearly identical results.\n",
        "  * As the sample size increases, the T-distribution approaches the normal distribution, and for large samples, the T-test statistic becomes virtually the same as the Z-test statistic.\n",
        "3. Difference in Distribution Used:\n",
        "  * Z-test uses the standard normal distribution (Z-distribution), which is appropriate when the population standard deviation is known or when the sample size is large enough to apply the Central Limit Theorem.\n",
        "  * T-test uses the T-distribution, which is more appropriate when the sample size is small, or the population standard deviation is unknown. The T-distribution has heavier tails than the normal distribution to account for the additional uncertainty with small sample sizes.\n",
        "4. Standard Deviation vs. Sample Standard Deviation:\n",
        "  * Z-test: The Z-test is used when the population standard deviation is known. This is often the case in situations where a large enough sample can give a reliable estimate of the population parameter.\n",
        "  * T-test: The T-test is used when the population standard deviation is unknown and must be estimated from the sample using the sample standard deviation.\n",
        "5. Sample Size Consideration:\n",
        "  * Z-test: The Z-test is typically used when the sample size is large (n≥30), allowing the sample mean to be approximately normally distributed according to the Central Limit Theorem .\n",
        "  * T-test: The T-test is specifically useful for small sample sizes (usually n < 30) where the sample standard deviation is the best estimate for the population standard deviation. For small samples, the variability of the sample mean is greater, and the T-distribution accounts for that.\n",
        "\n",
        "**Differences and Use of Each:**\n",
        "\n",
        "|Feature\t|Z-test\t|T-test|\n",
        "|-|||\n",
        "|Population Standard Deviation\t|Known or large sample size (n≥30)\t|Unknown, estimated from sample|\n",
        "|Sample Size\t|Large (n≥30)\t|Small (n < 30)|\n",
        "|Distribution Used\t|Standard Normal Distribution (Z-distribution)\t|T-distribution (depends on degrees of freedom)|\n",
        "|Critical Value Table\t|Z-table (for normal distribution)\t|t-table (for T-distribution)|\n",
        "|When to Use\t|Population standard deviation known, or large sample sizes\t|Population standard deviation unknown, small sample sizes|"
      ],
      "metadata": {
        "id": "vBBWRa679Lw4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 15. What is a confidence interval, and how is it used to interpret statistical results?\n",
        "**Ans** - A confidence interval is a range of values, derived from a sample statistic, that is used to estimate an unknown population parameter with a certain level of confidence. It provides a range of plausible values for the parameter, along with the degree of uncertainty associated with the estimate. Confidence intervals are widely used in statistics to express the reliability of an estimate and are typically reported alongside point estimates.\n",
        "\n",
        "**Elements of a Confidence Interval**:\n",
        "1. Point Estimate: The sample statistic that serves as the best estimate of the population parameter.\n",
        "2. Margin of Error: The range above and below the point estimate that reflects the uncertainty or variability in the estimate.\n",
        "3. Confidence Level: The probability that the interval will contain the true population parameter if the same sampling process is repeated multiple times. Common confidence levels are 90%, 95%, and 99%.\n",
        "\n",
        "The general form of a confidence interval for a population mean is:\n",
        "\n",
        "    CI = (Xi-Margin of Error,Xi+Margin of Error)\n",
        "Where:\n",
        "\n",
        "  * Xi = Sample mean\n",
        "  * Margin of Error = Critical value * Standard error\n",
        "\n",
        "**Confidence Intervals are used to interpret the statistical result:**\n",
        "1. Estimating Population Parameters:\n",
        "  * A confidence interval gives a range of values within which we can be confident that the true population parameter lies.\n",
        "  * For example, if we calculate a 95% confidence interval for the mean income of a population, the interval might be from Rs40,000 to Rs50,000. This means we are 95% confident that the true population mean income is somewhere between Rs40,000 and Rs50,000.\n",
        "2. Interpretation of Confidence Intervals:\n",
        "  * A 95% confidence interval means that if we were to take many random samples and calculate the confidence interval for each sample, approximately 95% of those intervals would contain the true population parameter. It does not mean there's a 95% chance that the true value lies within any given interval. The true value either lies in the interval or it does not.\n",
        " * The wider the interval, the less precise the estimate. A narrow confidence interval indicates a more precise estimate, whereas a wider interval suggests more uncertainty.\n",
        "3. Decision Making in Hypothesis Testing:\n",
        "  * Confidence intervals can also be used to make decisions about hypothesis tests. For example, if a null hypothesis value falls outside of the confidence interval, it can provide evidence to reject the null hypothesis.\n",
        "  * Example: If the 95% CI for the mean difference between two groups does not include zero, it suggests that there is a statistically significant difference between the two groups.\n",
        "4. Determining Statistical Significance:\n",
        "  * When the confidence interval for a population parameter does not include a specific value, it indicates that the result is statistically significant.\n",
        "  * If a 95% confidence interval for the difference between two group means is (5,15), you can conclude that the true difference is unlikely to be zero, suggesting a significant difference between the groups."
      ],
      "metadata": {
        "id": "y9P-GYaR9WBJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q16 . What is the margin of error, and how does it affect the confidence interval?\n",
        "**Ans** - The margin of error is the range within which you expect the true population parameter to fall, given a certain level of confidence. It quantifies the uncertainty in an estimate due to the sampling process and is a critical component of a confidence interval. The margin of error is added and subtracted from the sample statistic to create the confidence interval.\n",
        "\n",
        "**Margin of Error affects the Confidence Interval:**\n",
        "1. Confidence Interval Construction: The margin of error is added and subtracted from the sample statistic to create the confidence interval. For example, if the sample mean is 50, and the margin of error is 5, the confidence interval would be:\n",
        "        CI = (50-5,50+5) = (45,55)\n",
        "So, we would be confident that the true population mean lies between 45 and 55.\n",
        "2. Impact of Larger Margin of Error:\n",
        "* A larger margin of error results in a wider confidence interval, indicating more uncertainty about the true population parameter.\n",
        "* The margin of error increases when the sample size is small, the variability of the population is high, or when the confidence level is higher.\n",
        "3. Impact of Smaller Margin of Error:\n",
        "* A smaller margin of error results in a narrower confidence interval, indicating more precision in the estimate.\n",
        "* The margin of error decreases when the sample size increases or the variability in the population is low."
      ],
      "metadata": {
        "id": "JhIN-_hf9feY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 17. How is Bayes' Theorem used in statistics, and what is its significance?\n",
        "**Ans** - Bayes' Theorem is a fundamental concept in statistics and probability theory that provides a way to update the probability of a hypothesis based on new evidence. It is named after Thomas Bayes, who developed it in the 18th century. Bayes' Theorem allows us to revise our beliefs about the likelihood of a hypothesis being true, given both prior knowledge and new data.\n",
        "\n",
        "**Bayes' Theorem Is used in Statistics**:\n",
        "\n",
        "Bayes' Theorem is used in a variety of statistical methods and applications, including:\n",
        "1. Updating Beliefs:\n",
        "* Bayes' Theorem is particularly useful when we have prior knowledge about a situation and want to update that knowledge as new data becomes available.\n",
        "* Example: If we're diagnosing a disease, we might have a prior probability of the disease based on general population statistics. As we get new test results, we can update our belief about whether the person has the disease.\n",
        "2. Statistical Inference:\n",
        "* Bayes' Theorem is used in Bayesian statistics to estimate parameters or make decisions based on observed data. In Bayesian statistics, all unknowns are treated as random variables with prior distributions. The observed data is then used to update these priors to form posterior distributions.\n",
        "* Example: In regression analysis, we might use Bayes' Theorem to update our belief about the parameters of a model after observing data.\n",
        "3. Classification:\n",
        "* Bayes' Theorem forms the basis for Naive Bayes classifiers, which are widely used in machine learning for tasks like spam email detection, sentiment analysis, and document classification. The classifier computes the probability of a certain class given the features of the data using Bayes' Theorem.\n",
        "* Example: If we're classifying emails as spam or not spam, Bayes' Theorem can help update the probability of an email being spam based on its content.\n",
        "4. Decision Theory:\n",
        "* In decision theory, Bayes' Theorem helps in making decisions under uncertainty. By updating prior probabilities with new evidence, Bayes' Theorem helps in choosing the most optimal decision based on the updated beliefs.\n",
        "* Example: A business might use Bayes' Theorem to update their prediction of future sales based on current economic conditions or customer feedback.\n",
        "\n",
        "**Significance of Bayes' Theorem:**\n",
        "1. Incorporating Prior Knowledge:\n",
        "* One of the key strengths of Bayes' Theorem is its ability to incorporate prior knowledge or beliefs into the analysis. This is especially valuable when data is limited or when prior information is available from previous studies, expert opinions, or historical data.\n",
        "* Example: In a clinical trial, prior knowledge about the effectiveness of a drug from past studies can be combined with new data from the current trial to improve the accuracy of predictions.\n",
        "2. Dynamic Updating:\n",
        "* Bayes' Theorem allows for dynamic updating of probabilities as new data becomes available. This makes it a powerful tool for ongoing decision-making processes, especially in fields where data is continuously collected or when decisions need to be adjusted as new information arises.\n",
        "* Example: In finance, stock analysts may continuously update their predictions about the price of a stock as new market information is received.\n",
        "3. Dealing with Uncertainty:\n",
        "* Bayes' Theorem is particularly useful when dealing with uncertainty in statistical analysis. It provides a structured way to reason about the uncertainty of different outcomes, updating the probability distributions as more evidence is collected.\n",
        "* Example: In medical testing, Bayes' Theorem can help calculate the probability of a patient having a disease given the test results, even when the test is not perfect.\n",
        "4. Handling Complex Problems:\n",
        "* Bayes' Theorem is essential in dealing with complex, multi-layered problems where there are several interdependent variables and possible outcomes. It allows the incorporation of various sources of uncertainty and integrates them into a single model.\n",
        "* Example: In artificial intelligence, Bayesian networks are used to model complex dependencies between different variables in a probabilistic framework."
      ],
      "metadata": {
        "id": "cWGJTIw29puJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 18 . What is the Chi-square distribution, and when is it used?\n",
        "**Ans** - The Chi-square distribution is a probability distribution widely used in statistics, particularly in hypothesis testing and the analysis of categorical data. It is a special case of the gamma distribution and is related to the normal distribution.\n",
        "\n",
        "**Use of Chi-square Distribution**\n",
        "1. Categorical Data: The Chi-square distribution is most commonly used for analyzing categorical data, especially when data is divided into frequency counts or proportions in different categories.\n",
        "\n",
        "2. Large Sample Sizes: The Chi-square tests generally work best with larger sample sizes. With small sample sizes, the Chi-square test may not provide reliable results, and alternative tests like Fisher's exact test may be more appropriate.\n",
        "\n",
        "3. Assumption of Normality: The Chi-square test for variance assumes that the data comes from a normally distributed population. For tests of independence or goodness-of-fit, the data should consist of counts or frequencies in different categories.\n",
        "\n",
        "4. Testing Independence: When you want to test whether two categorical variables are related or independent, you use the Chi-square test for independence. This is often used in surveys or observational studies.\n",
        "\n",
        "5. Goodness-of-Fit: When you want to compare observed data to a specific expected distribution, such as comparing the observed frequencies of outcomes to the expected frequencies in a model.\n",
        "\n",
        "**Example of Chi-square Test for Independence**:\n",
        "\n",
        "Imagine we are testing whether there is an association between education level and employment status among a sample of individuals:\n",
        "\n",
        "|Education Level\t|Employed\t|Unemployed\t|Total|\n",
        "|-||||\n",
        "|High School\t|30\t|10\t|40|\n",
        "|College\t|50\t|10\t|60|\n",
        "|Graduate\t|40\t|20\t|60|\n",
        "|Total\t|120\t|40\t|160|\n"
      ],
      "metadata": {
        "id": "gPezHylV9x7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 19. What is the Chi-square goodness of fit test, and how is it applied?\n",
        "**Ans** - The Chi-square goodness-of-fit test is a statistical test used to determine how well an observed frequency distribution fits an expected frequency distribution. It is used when we want to compare the observed data to a specific theoretical or expected distribution, such as whether data fits a uniform distribution or follows a specific pattern.\n",
        "\n",
        "**Purpose of the Chi-square Goodness-of-Fit Test**:\n",
        "* The test helps to assess whether the observed categorical data fits the expected distribution. For example, we might want to determine whether a die is fair.\n",
        "* The test compares the frequencies of categories in the sample data to the expected frequencies, and if the differences between the two are large, it suggests that the observed data does not fit the expected distribution.\n",
        "\n",
        "**Assumptions of the Chi-square Goodness-of-Fit Test**:\n",
        "1. The data must be categorical.\n",
        "2. The observations should be independent.\n",
        "3. Each expected frequency should be at least 5 to ensure the test's validity.\n",
        "4. The sample size should be large enough to allow for meaningful statistical testing.\n",
        "\n",
        "**Chi-square Goodness-of-Fit Test**:\n",
        "\n",
        "Step 1: State the Hypotheses\n",
        "* Null Hypothesis: The observed data fits the expected distribution. In other words, the differences between the observed and expected frequencies are due to random chance.\n",
        "* Alternative Hypothesis: The observed data does not fit the expected distribution. The differences between the observed and expected frequencies are too large to be attributed to chance.\n",
        "\n",
        "Step 2: Collect Data\n",
        "\n",
        "We need to collect the observed frequencies of each category in the sample data. Additionally, we must define the expected frequencies for each category, which could come from a known distribution or theoretical model.\n",
        "\n",
        "Step 3: Calculate the Chi-square Statistic\n",
        "\n",
        "The Chi-square statistic (χ^2) measures the discrepancy between the observed and expected frequencies. The formula is:\n",
        "\n",
        "    χ^2 = ∑(Oi-Ei)^2/Ei\n",
        "Where:\n",
        "* Oi = observed frequency in the i-th category,\n",
        "* Ei = expected frequency in the i-th category,\n",
        "* The summation is over all categories.\n",
        "\n",
        "Step 4: Determine the Degrees of Freedom\n",
        "\n",
        "The degrees of freedom for the Chi-square goodness-of-fit test are given by:\n",
        "\n",
        "    df = k−1\n",
        "Where:\n",
        "* k = number of categories in the data.\n",
        "\n",
        "Note: If any parameters are estimated from the data, we must subtract 1 or more from the degrees of freedom to account for those estimates.\n",
        "\n",
        "Step 5: Find the Critical Value or P-value\n",
        "\n",
        "Once we calculate the Chi-square statistic, we need to compare it to a critical value from the Chi-square distribution table, based on our chosen significance level (α) and the degrees of freedom df.\n",
        "* If the calculated χ^2 statistic is greater than the critical value, we reject the null hypothesis.\n",
        "* If the calculated χ^2 statistic is smaller than the critical value, we fail to reject the null hypothesis.\n",
        "\n",
        "Step 6: Make a Decision\n",
        "* Reject the null hypothesis if the test statistic exceeds the critical value or if the p-value is less than α. This means the observed frequencies significantly differ from the expected frequencies, indicating that the data does not fit the expected distribution.\n",
        "* Fail to reject the null hypothesis if the test statistic is smaller than the critical value or if the p-value is greater than α. This means there is no significant difference, and the observed data likely fits the expected distribution.\n",
        "\n",
        "**Example of Chi-square Goodness-of-Fit Test**:\n",
        "\n",
        "Scenario: Fair Die Test\n",
        "\n",
        "We want to test whether a 6-sided die is fair, meaning each face has an equal probability of landing. We roll the die 60 times and record the following outcomes:\n",
        "\n",
        "|Die Face\t|Observed Frequency (O)|\n",
        "|-||\n",
        "|1\t|10|\n",
        "|2\t|12|\n",
        "|3\t|8|\n",
        "|4\t|11|\n",
        "|5\t|9|\n",
        "|6\t|10|\n",
        "\n",
        "Step 1: State the Hypotheses\n",
        "* Null hypothesis (H0): The die is fair.\n",
        "* Alternative hypothesis (H1): The die is not fair.\n",
        "\n",
        "Step 2: Collect Data\n",
        "* Observed frequencies (O): As shown in the table above.\n",
        "* Expected frequencies (E): If the die is fair, each face should appear with equal probability. So, the expected frequency for each face is:\n",
        "\n",
        "      Ei = Total Rolls/Number of Faces = 60/6 = 10\n",
        "\n",
        "Step 3: Calculate the Chi-square Statistic The Chi-square statistic is:\n",
        "\n",
        "    χ^2 = ∑(Oi-Ei)^2/Ei\n",
        "\n",
        "For each die face:\n",
        "* For face 1: (10-10)^2/10=0\n",
        "* For face 2: (12-10)^2/10=0.4\n",
        "* For face 3: (8-10)^2/10=0.4\n",
        "* For face 4: (11-10)^2/10=0.1\n",
        "* For face 5: (9-10)^2/10=0.1\n",
        "* For face 6: (10-10)^2/10=0\n",
        "\n",
        "Sum these values to get the Chi-square statistic:\n",
        "\n",
        "    χ^2 = 0+0.4+0.4+0.1+0.1+0 = 1.0\n",
        "Step 4: Determine Degrees of Freedom The degrees of freedom are:\n",
        "\n",
        "    df = k-1 = 6-1 = 5\n",
        "Step 5: Find the Critical Value or P-value At a significance level of α=0.05 and 5 degrees of freedom, we can find the critical value from the Chi-square distribution table, which is approximately 11.07.\n",
        "\n",
        "Alternatively, we can calculate the p-value using statistical software or a Chi-square distribution table. For this case, the p-value corresponding to a Chi-square statistic of 1.0 with 5 degrees of freedom is approximately 0.96.\n",
        "\n",
        "Step 6: Make a Decision Since the calculated Chi-square statistic (1.0) is much less than the critical value (11.07) and the p-value (0.96) is greater than α=0.05, we fail to reject the null hypothesis. This suggests that there is no significant difference between the observed and expected frequencies, and the die appears to be fair."
      ],
      "metadata": {
        "id": "YyTf9f_Y99-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 20. What is the F-distribution, and when is it used in hypothesis testing?\n",
        "**Ans** - The F-distribution is a probability distribution that arises in statistics when comparing variances or when conducting certain types of hypothesis tests, particularly those that involve variance analysis (ANOVA), regression analysis, or comparing multiple population variances.\n",
        "\n",
        "**Characteristics of the F-distribution**:\n",
        "1. Non-negative Values: The F-distribution only takes on positive values (F≥0). It is often used for testing hypotheses involving the ratio of variances between two or more groups.\n",
        "2. Skewed to the Right: The distribution is right-skewed. It starts at zero and stretches out to the right, with the degree of skewness decreasing as the degrees of freedom increase.\n",
        "3. Degrees of Freedom: The F-distribution is defined by two sets of degrees of freedom:\n",
        "* Numerator degrees of freedom (df1): The degrees of freedom associated with the variance in the numerator (usually the between-group variance in ANOVA).\n",
        "* Denominator degrees of freedom (df2): The degrees of freedom associated with the variance in the denominator (usually the within-group variance in ANOVA).\n",
        "4. Shape: The shape of the F-distribution depends on the numerator and denominator degrees of freedom. The distribution becomes more symmetric as both degrees of freedom increase.\n",
        "\n",
        "**Applications of the F-distribution**:\n",
        "\n",
        "The F-distribution is primarily used in the context of comparing variances and conducting hypothesis tests involving the ratio of variances. The most common applications of the F-distribution are in:\n",
        "\n",
        "1. Analysis of Variance (ANOVA):\n",
        "* ANOVA is a statistical technique used to compare the means of three or more groups to see if at least one group mean is different from the others. In ANOVA, the F-distribution is used to calculate the F-statistic, which is the ratio of variances between groups.\n",
        "* In ANOVA, the null hypothesis (H0) usually states that all group means are equal, and the alternative hypothesis (H1) states that at least one group mean is different.\n",
        "* The F-statistic is calculated as the ratio of the between-group variance to the within-group variance:\n",
        "\n",
        "      F = variance between groups/variance within groups\n",
        "* If the calculated F-statistic is large, it suggests that there is a significant difference between the group means, and the null hypothesis is rejected. If the F-statistic is small, the null hypothesis is not rejected.\n",
        "\n",
        "Example: Testing whether the average scores of students in three different teaching methods (Group A, Group B, Group C) are equal.\n",
        "\n",
        "2. Comparing Two Variances (F-test for Equality of Variances):\n",
        "* The F-distribution is used in the F-test to compare the variances of two populations. This is done by calculating the ratio of the two sample variances.\n",
        "* The null hypothesis (H0) typically states that the variances of the two populations are equal, and the alternative hypothesis (H1) states that the variances are not equal.\n",
        "\n",
        "3. Regression Analysis:\n",
        "* The F-distribution is used in multiple regression analysis to test the significance of the overall regression model. The F-test for regression tests whether the regression model as a whole is a good fit for the data.\n",
        "* The F-statistic in regression analysis is the ratio of the explained variance to the unexplained variance:\n",
        "\n",
        "      F = Explained Variance/Unexplained Variance\n",
        "\n",
        "* If the F-statistic is large, it suggests that the model explains a significant proportion of the variance in the dependent variable, and the model is a good fit.\n",
        "\n",
        "**Formula for the F-statistic**:\n",
        "\n",
        "The formula for the F-statistic depends on the specific test being conducted (ANOVA, comparing variances, regression), but generally, it is:\n",
        "\n",
        "    F=Variance between groups (or explained variance)/\n",
        "Variance within groups (or unexplained variance)\n",
        "\n",
        "In ANOVA, for example, it is:\n",
        "\n",
        "    F= Mean square between groups (MSB)/Mean square within groups (MSW)\n",
        "\n",
        "Where:\n",
        "* MSB = (Mean Square Between Groups),\n",
        "* MSW = (Mean Square Within Groups),\n",
        "* SSB = sum of squares between groups,\n",
        "* SSW = sum of squares within groups,\n",
        "* df1 and df2 are the degrees of freedom for the between-group and within-group variances, respectively.\n",
        "\n",
        "**Hypothesis for the F-test**:\n",
        "* Null Hypothesis (H0): The variances being compared are equal.\n",
        "* Alternative Hypothesis (H1): The variances being compared are not equal.\n",
        "\n",
        "**Example: One-way ANOVA**\n",
        "\n",
        "Suppose we are conducting an experiment to test whether three different fertilizers lead to different average plant growth. we collect data and have three groups: Fertilizer A, Fertilizer B, and Fertilizer C.\n",
        "* Null hypothesis: The mean plant growth for all three fertilizers is equal.\n",
        "* Alternative hypothesis: At least one fertilizer leads to a different mean plant growth.\n",
        "* Calculate the F-statistic using the ratio of the variance between the groups to the variance within the groups.\n",
        "* Compare the F-statistic to the critical value from the F-distribution to make a decision."
      ],
      "metadata": {
        "id": "_JCbiX3T-H4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 21. What is an ANOVA test, and what are its assumptions?\n",
        "**Ans** - ANOVA is a statistical method used to compare the means of three or more groups to determine if at least one group mean is significantly different from the others. It is primarily used to test hypotheses about group differences based on sample data.\n",
        "\n",
        "The core idea behind ANOVA is to partition the total variability in the data into components that can be attributed to different sources. These sources typically include:\n",
        "\n",
        "1. Between-group variability: The variation due to differences in the means of different groups.\n",
        "2. Within-group variability: The variation within each group, often referred to as the error or residual variance.\n",
        "\n",
        "ANOVA compares these two sources of variation and uses the F-statistic to determine if the differences between the group means are statistically significant.\n",
        "\n",
        "**Null and Alternative Hypotheses**:\n",
        "* Null Hypothesis (H0): The means of all groups are equal.\n",
        "* Alternative Hypothesis (H1): At least one of the group means is different from the others.\n",
        "\n",
        "**Assumptions of ANOVA**:\n",
        "\n",
        "For ANOVA to provide valid results, several assumptions need to be met. These assumptions ensure that the test statistics follow the appropriate distributions.\n",
        "1. Independence of Observations:\n",
        "* The data points within each group should be independent of one another. That is, the measurements of one subject or group should not influence the measurements of another subject or group.\n",
        "* This assumption is crucial for the validity of the results.\n",
        "2. Normality:\n",
        "* The data within each group should be approximately normally distributed. ANOVA is fairly robust to violations of normality when sample sizes are large, but small sample sizes may require a more stringent check for normality.\n",
        "* Normality can be checked visually with histograms or Q-Q plots, or through statistical tests like the Shapiro-Wilk test.\n",
        "3. Homogeneity of Variances:\n",
        "* The variances within each group should be approximately equal. This assumption is important because if the variances are unequal, the F-statistic might not follow an F-distribution.\n",
        "* Homogeneity of variances can be tested using Levene's test, Bartlett's test, or other similar methods.\n",
        "4. Random Sampling:\n",
        "* The data should come from random samples to ensure that the groups are representative of the population."
      ],
      "metadata": {
        "id": "9Knga2Ot-U26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 22. What are the different types of ANOVA tests?\n",
        "**Ans** - There are several types of ANOVA tests, each designed to handle different experimental designs and hypotheses about group means. The primary types are:\n",
        "1. **One-Way ANOVA**\n",
        "Purpose: Compares the means of three or more independent groups based on one independent variable.\n",
        "* When to use:\n",
        "  * We have one categorical independent variable with more than two levels and one continuous dependent variable.\n",
        "  * We want to test whether there is a statistically significant difference between the means of the groups.\n",
        "* Example: Comparing the average test scores of students from three different teaching methods (A, B, and C).\n",
        "* Null Hypothesis (H0): The means of all groups are equal.\n",
        "* Alternative Hypothesis (H1): At least one group mean is different.\n",
        "\n",
        "2. **Two-Way ANOVA**\n",
        "Purpose: Compares the means of groups based on two independent variables. It also evaluates the interaction between the two independent variables on the dependent variable.\n",
        "* When to use:\n",
        "  * We have two categorical independent variables and one continuous dependent variable.\n",
        "  * We want to assess the individual effects of each independent variable on the dependent variable, as well as whether there is an interaction between the two factors.\n",
        "* Example: Testing the effect of teaching method (A, B, C) and gender (Male, Female) on students' test scores.\n",
        "* Null Hypothesis (H0): There are no main effects and no interaction between the two factors.\n",
        "  * H0 for main effects: The means of each level of the independent variables are equal.\n",
        "  * H0 for interaction: There is no interaction effect between the independent variables.\n",
        "* Alternative Hypothesis (H1): At least one of the following is true:\n",
        "  * There is a significant main effect of at least one independent variable.\n",
        "  * There is a significant interaction effect between the two independent variables.\n",
        "* Two types of main effects in this case:\n",
        "  * Main effect of teaching method (ignoring gender).\n",
        "  * Main effect of gender (ignoring teaching method).\n",
        "  * Interaction effect between teaching method and gender.\n",
        "\n",
        "3. **Repeated Measures ANOVA**\n",
        "Purpose: Used when the same subjects are measured multiple times under different conditions, such as when the same participants are tested at different time points.\n",
        "* When to use:\n",
        "  * We have one categorical independent variable and a repeated measure of the same subjects.\n",
        "  * We want to examine whether the means of the same participants, when measured under different conditions or over time, differ significantly.\n",
        "* Example: Measuring the blood pressure of patients at three different times.\n",
        "* Null Hypothesis (H0): The means of the repeated measures are equal.\n",
        "* Alternative Hypothesis (H1): At least one of the means is different.\n",
        "* Key feature: Since measurements are taken from the same individuals, this type of ANOVA accounts for the correlation between the repeated measures.\n",
        "\n",
        "4. **Multivariate Analysis of Variance (MANOVA)**\n",
        "Purpose: Extends ANOVA to handle multiple dependent variables simultaneously. It assesses the influence of one or more independent variables on multiple dependent variables.\n",
        "* When to use:\n",
        "  * We have more than one dependent variable that is measured on the same set of groups, and we want to test for differences in the means across those dependent variables simultaneously.\n",
        "* Example: Testing the effect of different teaching methods on students' test scores in multiple subjects.\n",
        "* Null Hypothesis (H0): There is no significant difference in the mean vectors of the dependent variables across the groups.\n",
        "* Alternative Hypothesis (H1): At least one group differs on one or more of the dependent variables.\n",
        "\n",
        "5. **Analysis of Covariance (ANCOVA)**\n",
        "Purpose: Combines ANOVA and regression to examine the effect of categorical independent variables on a continuous dependent variable while controlling for one or more continuous covariates (confounding variables).\n",
        "* When to use:\n",
        "  * We want to test group differences (like ANOVA), but we need to control for the influence of one or more continuous variables that might affect the dependent variable.\n",
        "* Example: Testing the effect of different teaching methods on student performance while controlling for baseline academic performance.\n",
        "* Null Hypothesis (H0): The means of the dependent variable are equal across groups after adjusting for the covariates.\n",
        "* Alternative Hypothesis (H1): At least one group mean differs after adjusting for the covariates.\n",
        "\n",
        "6. **Mixed-Design ANOVA**\n",
        "Purpose: Combines features of both between-subjects and within-subjects designs. It is used when there are both repeated measures (within-subjects) and independent groups (between-subjects) factors.\n",
        "* When to use:\n",
        "  * We have both a between-subjects factor and a within-subjects factor.\n",
        "* Example: Testing the effect of a new drug on blood pressure over time with measurements taken at multiple time points.\n",
        "* Null Hypothesis (H0): There are no main effects or interactions between the factors.\n",
        "* Alternative Hypothesis (H1): There are significant main effects or interactions.\n",
        "\n",
        "**Types of ANOVA:**\n",
        "\n",
        "|Type of ANOVA\t|Purpose\t|Example|\n",
        "|-|||\n",
        "|One-Way ANOVA\t|Compares means of three or more groups with one independent variable\t|Effect of teaching methods (A, B, C) on student scores|\n",
        "|Two-Way ANOVA\t|Compares means of groups based on two independent variables, assesses interaction effects\t|Teaching method and gender on student performance|\n",
        "|Repeated Measures ANOVA\t|Compares means across repeated measurements on the same subjects\t|Measuring blood pressure before, during, and after treatment|\n",
        "|MANOVA\t|Compares means across groups for multiple dependent variables simultaneously\t|Teaching methods and their effects on multiple subjects|\n",
        "|ANCOVA\t|Adjusts for covariates when comparing means between groups\t|Teaching methods' effects on test scores, controlling for prior performance|\n",
        "|Mixed-Design ANOVA\t|Combines between-subjects and within-subjects factors, assesses both types of effects\t|Effect of drug on blood pressure over time|"
      ],
      "metadata": {
        "id": "OWZRo4lT-dWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 23. What is the F-test, and how does it relate to hypothesis testing?\n",
        "**Ans** - The F-test is a statistical test used to compare variances between two or more groups or to assess the overall significance of a regression model. The F-test is based on the F-distribution, which is a ratio of two variances.\n",
        "\n",
        "**F-test Relate to Hypothesis Testing**\n",
        "\n",
        "In hypothesis testing, the F-test is used to test the null hypothesis by comparing variances or assessing the significance of model fit. Specifically, the F-statistic is calculated as the ratio of two different estimates of variance:\n",
        "\n",
        "    F = Variance Between Groups/Variance Within Groups\n",
        "\n",
        "Null and Alternative Hypotheses in the F-test:\n",
        "1. F-test for comparing two variances:\n",
        "  * Null Hypothesis (H0): The two population variances are equal.\n",
        "  * Alternative Hypothesis (H1): The two population variances are not equal.\n",
        "2. F-test in ANOVA (used to compare multiple group means):\n",
        "  * Null Hypothesis (H0): All group means are equal, implying no significant differences between groups.\n",
        "  * Alternative Hypothesis (H1): At least one group mean is different.\n",
        "3. F-test in regression (to test model significance):\n",
        "  * Null Hypothesis (H0): The regression model explains no more variance than the mean of the dependent variable.\n",
        "  * Alternative Hypothesis (H1): The regression model explains significant variance in the dependent variable.\n",
        "\n",
        "**Calculating the F-statistic**\n",
        "\n",
        "The F-statistic is calculated using the ratio of two variances:\n",
        "\n",
        "    F = Mean Square Between (MSB)/Mean Square Within (MSW)\n",
        "Where:\n",
        "* MSB is the Mean Square Between groups.\n",
        "* MSW is the Mean Square Within groups."
      ],
      "metadata": {
        "id": "We1TkafY-42S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical Questions"
      ],
      "metadata": {
        "id": "CHLcNBxD_Dbi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 1. Write a Python program to perform a Z-test for comparing a sample mean to a known population mean and interpret the results.\n",
        "**Ans** - Python program to perform a Z-test for comparing a sample mean to a known population mean."
      ],
      "metadata": {
        "id": "y7M7svl4_Ksd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "sample_data = [120, 130, 125, 140, 135, 150, 145, 130, 125, 140]\n",
        "population_mean = 130\n",
        "population_std_dev = 10\n",
        "sample_size = len(sample_data)\n",
        "sample_mean = np.mean(sample_data)\n",
        "sem = population_std_dev / np.sqrt(sample_size)\n",
        "z_score = (sample_mean - population_mean) / sem\n",
        "p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))\n",
        "alpha = 0.05\n",
        "print(f\"Sample Mean: {sample_mean}\")\n",
        "print(f\"Z-score: {z_score}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: The sample mean is significantly different from the population mean.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The sample mean is not significantly different from the population mean.\")"
      ],
      "metadata": {
        "id": "RI7TxRjyP75h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 2. Simulate random data to perform hypothesis testing and calculate the corresponding P-value using Python.\n",
        "**Ans** - To simulate random data and perform hypothesis testing using Python, we'll use a t-test, but the procedure would be similar for a Z-test or other hypothesis tests.\n",
        "\n",
        "Steps:\n",
        "1. Simulate Random Data: Generate a random sample using a normal distribution.\n",
        "2. Set up Hypotheses: We'll test the null hypothesis that the sample mean is equal to a known population mean.\n",
        "3. Perform the t-test: Use the scipy.stats.ttest_1samp function to perform the test.\n",
        "4. Calculate the P-value: The ttest_1samp function also gives us the P-value."
      ],
      "metadata": {
        "id": "W7X7xN7i_Vt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "np.random.seed(42)\n",
        "sample_size = 30\n",
        "population_mean = 50\n",
        "sample_data = np.random.normal(loc=52, scale=10, size=sample_size)\n",
        "t_stat, p_value = stats.ttest_1samp(sample_data, population_mean)\n",
        "print(f\"Sample Data: {sample_data[:10]}...\")\n",
        "print(f\"Sample Mean: {np.mean(sample_data)}\")\n",
        "print(f\"T-statistic: {t_stat}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: The sample mean is significantly different from the population mean.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The sample mean is not significantly different from the population mean.\")"
      ],
      "metadata": {
        "id": "uD-ERDFvTCIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 3. Implement a one-sample Z-test using Python to compare the sample mean with the population mean."
      ],
      "metadata": {
        "id": "4AjdPYqd_dHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "np.random.seed(42)\n",
        "sample_size = 30\n",
        "population_mean = 50\n",
        "population_std_dev = 10\n",
        "\n",
        "sample_data = np.random.normal(loc=52, scale=10, size=sample_size)\n",
        "\n",
        "sample_mean = np.mean(sample_data)\n",
        "sem = population_std_dev / np.sqrt(sample_size)\n",
        "\n",
        "z_score = (sample_mean - population_mean) / sem\n",
        "\n",
        "p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))\n",
        "\n",
        "print(f\"Sample Data: {sample_data[:10]}...\")\n",
        "print(f\"Sample Mean: {sample_mean}\")\n",
        "print(f\"Z-score: {z_score}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "alpha = 0.05\n",
        "\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: The sample mean is significantly different from the population mean.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The sample mean is not significantly different from the population mean.\")"
      ],
      "metadata": {
        "id": "S5qAMpuHT3-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 4. Perform a two-tailed Z-test using Python and visualize the decision region on a plot.\n"
      ],
      "metadata": {
        "id": "5fX3Ndhp_lvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "np.random.seed(42)\n",
        "sample_size = 30\n",
        "population_mean = 50\n",
        "population_std_dev = 10\n",
        "\n",
        "sample_data = np.random.normal(loc=52, scale=10, size=sample_size)\n",
        "\n",
        "sample_mean = np.mean(sample_data)\n",
        "sem = population_std_dev / np.sqrt(sample_size)\n",
        "\n",
        "z_score = (sample_mean - population_mean) / sem\n",
        "\n",
        "p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))\n",
        "\n",
        "print(f\"Sample Mean: {sample_mean}\")\n",
        "print(f\"Z-score: {z_score}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "alpha = 0.05\n",
        "\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: The sample mean is significantly different from the population mean.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The sample mean is not significantly different from the population mean.\")\n",
        "\n",
        "z_values = np.linspace(-4, 4, 1000)\n",
        "z_pdf = stats.norm.pdf(z_values)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(z_values, z_pdf, label='Z-distribution', color='blue')\n",
        "\n",
        "plt.fill_between(z_values, z_pdf, where=(z_values <= -stats.norm.ppf(1 - alpha / 2)) | (z_values >= stats.norm.ppf(1 - alpha / 2)), color='red', alpha=0.5, label='Rejection Region')\n",
        "\n",
        "plt.axvline(x=z_score, color='green', linestyle='dashed', label=f\"Z-score: {z_score:.2f}\")\n",
        "\n",
        "critical_value = stats.norm.ppf(1 - alpha / 2)\n",
        "plt.axvline(x=critical_value, color='black', linestyle='dashed', label=f\"Critical Value: ±{critical_value:.2f}\")\n",
        "\n",
        "plt.title('Two-Tailed Z-test with Decision Region')\n",
        "plt.xlabel('Z-score')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6LLqPNz6Utq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 5. Create a Python function that calculates and visualizes Type 1 and Type 2 errors during hypothesis testing."
      ],
      "metadata": {
        "id": "XtSgEZUg_uWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "def plot_type_1_and_type_2_errors(population_mean, sample_mean, population_std_dev, alpha, sample_size, effect_size):\n",
        "    z_critical = stats.norm.ppf(1 - alpha / 2)\n",
        "\n",
        "    sem = population_std_dev / np.sqrt(sample_size)\n",
        "\n",
        "    x_values = np.linspace(population_mean - 4 * population_std_dev, population_mean + 4 * population_std_dev, 1000)\n",
        "    null_distribution = stats.norm.pdf(x_values, loc=population_mean, scale=population_std_dev)\n",
        "\n",
        "    alt_distribution = stats.norm.pdf(x_values, loc=population_mean + effect_size, scale=population_std_dev)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(x_values, null_distribution, label=\"Null Hypothesis (H0)\", color='blue')\n",
        "    plt.plot(x_values, alt_distribution, label=\"Alternative Hypothesis (H1)\", color='orange')\n",
        "\n",
        "    plt.fill_between(x_values, null_distribution, where=(x_values <= -z_critical * sem + population_mean) |\n",
        "                                            (x_values >= z_critical * sem + population_mean), color='red', alpha=0.5, label='Type 1 Error Region')\n",
        "\n",
        "    type_2_left = stats.norm.cdf(-z_critical, loc=effect_size, scale=sem)\n",
        "    type_2_right = 1 - stats.norm.cdf(z_critical, loc=effect_size, scale=sem)\n",
        "    plt.fill_between(x_values, alt_distribution, where=(x_values <= -z_critical + population_mean + effect_size) |\n",
        "                                             (x_values >= z_critical + population_mean + effect_size), color='yellow', alpha=0.5, label='Type 2 Error Region')\n",
        "\n",
        "    plt.title('Type 1 and Type 2 Errors in Hypothesis Testing')\n",
        "    plt.xlabel('Sample Mean')\n",
        "    plt.ylabel('Probability Density')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "population_mean = 50\n",
        "sample_mean = 52\n",
        "population_std_dev = 10\n",
        "alpha = 0.05\n",
        "sample_size = 30\n",
        "effect_size = 2\n",
        "\n",
        "plot_type_1_and_type_2_errors(population_mean, sample_mean, population_std_dev, alpha, sample_size, effect_size)"
      ],
      "metadata": {
        "id": "NDB3XooIY7wH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 6. Write a Python program to perform an independent T-test and interpret the results."
      ],
      "metadata": {
        "id": "ZLQTf3U__w6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "def independent_t_test(sample1, sample2, alpha=0.05):\n",
        "\n",
        "    t_stat, p_value = stats.ttest_ind(sample1, sample2)\n",
        "\n",
        "    print(f\"T-statistic: {t_stat}\")\n",
        "    print(f\"P-value: {p_value}\")\n",
        "\n",
        "    if p_value < alpha:\n",
        "        print(\"Reject the null hypothesis: There is a significant difference between the two groups.\")\n",
        "    else:\n",
        "        print(\"Fail to reject the null hypothesis: There is no significant difference between the two groups.\")\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "sample1 = np.random.normal(loc=50, scale=10, size=30)sample2 = np.random.normal(loc=55, scale=12, size=30)\n",
        "\n",
        "independent_t_test(sample1, sample2)"
      ],
      "metadata": {
        "id": "47c6QGCCatzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Simulate Data:\n",
        "* sample1 and sample2 are generated using np.random.normal() with different means (50 and 55) and standard deviations (10 and 12).\n",
        "2. T-test Calculation:\n",
        "* We use scipy.stats.ttest_ind(sample1, sample2) to perform the independent T-test. This function returns the t-statistic and p-value.\n",
        "3. Interpretation:\n",
        "* If the p-value is less than the significance level (α=0.05), we reject the null hypothesis, suggesting that there is a significant difference between the two groups.\n",
        "* If the p-value is greater than α, we fail to reject the null hypothesis, indicating that there is no significant difference."
      ],
      "metadata": {
        "id": "QCiYHg9udOqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 7. Perform a paired sample T-test using Python and visualize the comparison results."
      ],
      "metadata": {
        "id": "Dx0AMDvP_zGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "def paired_t_test(before, after, alpha=0.05):\n",
        "    t_stat, p_value = stats.ttest_rel(before, after)\n",
        "\n",
        "    print(f\"T-statistic: {t_stat}\")\n",
        "    print(f\"P-value: {p_value}\")\n",
        "\n",
        "    if p_value < alpha:\n",
        "        print(\"Reject the null hypothesis: There is a significant difference between the paired samples.\")\n",
        "    else:\n",
        "        print(\"Fail to reject the null hypothesis: There is no significant difference between the paired samples.\")\n",
        "\n",
        "    return t_stat, p_value\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "before = np.random.normal(loc=50, scale=10, size=30)\n",
        "after = before + np.random.normal(loc=5, scale=3, size=30)\n",
        "\n",
        "t_stat, p_value = paired_t_test(before, after)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "\n",
        "plt.plot(range(len(before)), before, 'bo-', label='Before', alpha=0.7)\n",
        "plt.plot(range(len(after)), after, 'ro-', label='After', alpha=0.7)\n",
        "\n",
        "plt.title('Comparison of Paired Samples (Before vs. After)')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Measurement Value')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "naPFYDTXdxOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualization:**\n",
        "\n",
        "The plot will show:\n",
        "* Before measurements (in blue).\n",
        "* After measurements (in red).\n",
        "* Each pair is connected by a line to show the change in measurements for each subject.\n",
        "\n",
        "**Key Points:**\n",
        "* The paired T-test is used when we have two measurements from the same group or individual, such as before and after an intervention.\n",
        "* This test assumes that the differences between the paired observations are normally distributed.\n",
        "* Visualizing the paired data helps to better understand the magnitude and direction of changes.\n",
        "\n",
        "This Python program performs the paired sample T-test and visualizes the comparison between paired data, making it easier to interpret the statistical results."
      ],
      "metadata": {
        "id": "lJnTbFFPkaCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 8. Simulate data and perform both Z-test and T-test, then compare the results using Python."
      ],
      "metadata": {
        "id": "uP0-5J6V_0t3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def z_test(sample, population_mean, population_std_dev, alpha=0.05):\n",
        "    sample_mean = np.mean(sample)\n",
        "    sample_size = len(sample)\n",
        "    z_stat = (sample_mean - population_mean) / (population_std_dev / np.sqrt(sample_size))\n",
        "    p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))\n",
        "\n",
        "    return z_stat, p_value\n",
        "\n",
        "def t_test(sample, population_mean, alpha=0.05):\n",
        "    sample_mean = np.mean(sample)\n",
        "    sample_std_dev = np.std(sample, ddof=1)\n",
        "    sample_size = len(sample)\n",
        "    t_stat, p_value = stats.ttest_1samp(sample, population_mean)\n",
        "\n",
        "    return t_stat, p_value\n",
        "\n",
        "def simulate_data(population_mean, population_std_dev, sample_size):\n",
        "    return np.random.normal(loc=population_mean, scale=population_std_dev, size=sample_size)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "population_mean = 50\n",
        "population_std_dev = 10\n",
        "sample_size = 30\n",
        "\n",
        "sample_data = simulate_data(population_mean, population_std_dev, sample_size)\n",
        "\n",
        "z_stat, z_p_value = z_test(sample_data, population_mean, population_std_dev)\n",
        "print(f\"Z-test: Z-statistic = {z_stat}, P-value = {z_p_value}\")\n",
        "\n",
        "t_stat, t_p_value = t_test(sample_data, population_mean)\n",
        "print(f\"T-test: T-statistic = {t_stat}, P-value = {t_p_value}\")\n",
        "\n",
        "if z_p_value < 0.05:\n",
        "    print(\"Z-test: Reject the null hypothesis.\")\n",
        "else:\n",
        "    print(\"Z-test: Fail to reject the null hypothesis.\")\n",
        "\n",
        "if t_p_value < 0.05:\n",
        "    print(\"T-test: Reject the null hypothesis.\")\n",
        "else:\n",
        "    print(\"T-test: Fail to reject the null hypothesis.\")\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.hist(sample_data, bins=15, color='skyblue', edgecolor='black', alpha=0.7, label=\"Sample Data\")\n",
        "plt.axvline(x=population_mean, color='r', linestyle='--', label=\"Population Mean\")\n",
        "plt.title('Histogram of Simulated Sample Data')\n",
        "plt.xlabel('Data Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "219uHzK0mT48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 9. Write a Python function to calculate the confidence interval for a sample mean and explain its significance."
      ],
      "metadata": {
        "id": "dMenT2ql_2I3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "def calculate_confidence_interval(sample, confidence_level=0.95):\n",
        "    sample_mean = np.mean(sample)\n",
        "    sample_std = np.std(sample, ddof=1)\n",
        "    sample_size = len(sample)\n",
        "\n",
        "    t_score = stats.t.ppf((1 + confidence_level) / 2, df=sample_size - 1)\n",
        "\n",
        "    margin_of_error = t_score * (sample_std / np.sqrt(sample_size))\n",
        "\n",
        "    lower_bound = sample_mean - margin_of_error\n",
        "    upper_bound = sample_mean + margin_of_error\n",
        "\n",
        "    return lower_bound, upper_bound\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "population_mean = 50\n",
        "population_std_dev = 10\n",
        "sample_size = 30\n",
        "sample_data = np.random.normal(loc=population_mean, scale=population_std_dev, size=sample_size)\n",
        "\n",
        "lower, upper = calculate_confidence_interval(sample_data, confidence_level=0.95)\n",
        "print(f\"95% Confidence Interval: ({lower:.2f}, {upper:.2f})\")"
      ],
      "metadata": {
        "id": "-rEtIjT5pFOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Significance of Confidence Interval:**\n",
        "* A confidence interval gives us a range of plausible values for the population mean.\n",
        "* A wider confidence interval indicates more uncertainty about the population mean, while a narrower confidence interval indicates more precision.\n",
        "* The confidence level (e.g., 95%, 99%) represents how confident we are that the interval contains the true population mean."
      ],
      "metadata": {
        "id": "N4F_aQvlqUP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 10. Write a Python program to calculate the margin of error for a given confidence level using sample data."
      ],
      "metadata": {
        "id": "PmsMxOeZ_3X3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "def calculate_margin_of_error(sample, confidence_level=0.95):\n",
        "    sample_mean = np.mean(sample)\n",
        "    sample_std = np.std(sample, ddof=1)\n",
        "    sample_size = len(sample)\n",
        "\n",
        "    t_score = stats.t.ppf((1 + confidence_level) / 2, df=sample_size - 1)\n",
        "\n",
        "    margin_of_error = t_score * (sample_std / np.sqrt(sample_size))\n",
        "\n",
        "    return margin_of_error\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "population_mean = 50\n",
        "population_std_dev = 10\n",
        "sample_size = 30\n",
        "sample_data = np.random.normal(loc=population_mean, scale=population_std_dev, size=sample_size)\n",
        "\n",
        "margin_of_error = calculate_margin_of_error(sample_data, confidence_level=0.95)\n",
        "print(f\"Margin of Error: {margin_of_error:.2f}\")"
      ],
      "metadata": {
        "id": "W_pnseBLrMrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 11. Implement a Bayesian inference method using Bayes' Theorem in Python and explain the process.\n",
        "**Ans** - Bayesian inference is a method of statistical inference in which Bayes' Theorem is used to update the probability of a hypothesis based on new evidence or data. Bayes' Theorem combines prior knowledge (prior probability) and new data (likelihood) to produce an updated probability (posterior probability).\n",
        "\n",
        "**Bayes' Theorem Formula:**\n",
        "\n",
        "The formula for Bayes' Theorem is:\n",
        "\n",
        "    P(H|D) = P(D|H)*P(H)/P(D)\n",
        "Where:\n",
        "* P(H|D) is the posterior probability, or the probability of the hypothesis H being true given the data D.\n",
        "* P(D|H) is the likelihood, or the probability of observing the data D given that the hypothesis H is true.\n",
        "* P(H) is the prior probability, or the initial belief about the hypothesis before seeing the data.\n",
        "* P(D) is the evidence, or the total probability of observing the data D across all possible hypotheses. It ensures the probabilities are properly normalized.\n",
        "\n",
        "**Bayesian Inference Process:**\n",
        "1. Prior Probability P(H): This is our initial belief about the hypothesis before observing the data.\n",
        "2. Likelihood P(D|H): This is the likelihood of observing the data given the hypothesis.\n",
        "3. Posterior Probability P(H|D): This is the updated probability of the hypothesis after observing the data, which we are interested in.\n",
        "4. Evidence P(D): This is the total probability of the data, which can be found by summing over all hypotheses."
      ],
      "metadata": {
        "id": "fZXtrwKn_7Vz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def bayesian_inference(prior_h1, prior_h2, likelihood_h1, likelihood_h2, evidence):\n",
        "    posterior_h1 = (likelihood_h1 * prior_h1) / evidence\n",
        "    posterior_h2 = (likelihood_h2 * prior_h2) / evidence\n",
        "\n",
        "    return posterior_h1, posterior_h2\n",
        "\n",
        "def calculate_likelihood(data, p_head):\n",
        "    heads = np.sum(data)\n",
        "    tails = len(data) - heads\n",
        "    likelihood = (p_head ** heads) * ((1 - p_head) ** tails)\n",
        "    return likelihood\n",
        "\n",
        "def main():\n",
        "    prior_h1 = 0.5\n",
        "    prior_h2 = 0.5\n",
        "\n",
        "    data = np.array([1, 1, 1, 1, 1, 0, 1, 1, 0, 1])\n",
        "\n",
        "    likelihood_h1 = calculate_likelihood(data, p_head=0.7)\n",
        "    likelihood_h2 = calculate_likelihood(data, p_head=0.5)\n",
        "\n",
        "    evidence = (likelihood_h1 * prior_h1) + (likelihood_h2 * prior_h2)\n",
        "\n",
        "    posterior_h1, posterior_h2 = bayesian_inference(prior_h1, prior_h2, likelihood_h1, likelihood_h2, evidence)\n",
        "\n",
        "    print(f\"Posterior Probability of H1 (biased coin): {posterior_h1:.4f}\")\n",
        "    print(f\"Posterior Probability of H2 (fair coin): {posterior_h2:.4f}\")\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "-0RHMJS_s25X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Process of Bayesian Inference:**\n",
        "1. Prior Probabilities: We start with an initial belief about the hypotheses (whether the coin is biased or fair).\n",
        "2. Likelihood: We calculate how likely the observed data is under each hypothesis.\n",
        "3. Bayes' Theorem: We update our beliefs based on the likelihood and the prior probabilities using Bayes' Theorem.\n",
        "4. Posterior: The updated probability, called the posterior, reflects our revised belief about the hypothesis after considering the data.\n",
        "\n",
        "**Significance of Bayesian Inference:**\n",
        "* Bayesian inference allows us to update our beliefs as new data arrives, providing a dynamic and flexible approach to statistical analysis.\n",
        "* It can be particularly useful when dealing with uncertain or incomplete information and when prior knowledge is available."
      ],
      "metadata": {
        "id": "un8Jy_V3xJqF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 12. Perform a Chi-square test for independence between two categorical variables in Python."
      ],
      "metadata": {
        "id": "gy0HfbUG_9pA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "data = {'Coffee': [30, 10],\n",
        "        'Tea': [20, 40]}\n",
        "\n",
        "df = pd.DataFrame(data, index=['Male', 'Female'])\n",
        "\n",
        "chi2, p_value, dof, expected = chi2_contingency(df)\n",
        "\n",
        "print(f\"Chi-Square Statistic: {chi2:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "print(f\"Degrees of Freedom: {dof}\")\n",
        "print(f\"Expected Frequencies:\\n{expected}\")"
      ],
      "metadata": {
        "id": "GS77Ar_RyNA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 13. Write a Python program to calculate the expected frequencies for a Chi-square test based on observed data."
      ],
      "metadata": {
        "id": "S8pHQFCWAAF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_expected_frequencies(observed_data):\n",
        "    row_sums = np.sum(observed_data, axis=1, keepdims=True)\n",
        "    col_sums = np.sum(observed_data, axis=0, keepdims=True)\n",
        "    grand_total = np.sum(observed_data)\n",
        "\n",
        "    expected_data = (row_sums @ col_sums) / grand_total\n",
        "\n",
        "    return expected_data\n",
        "\n",
        "observed_data = np.array([[30, 10],\n",
        "                          [20, 40]])\n",
        "\n",
        "expected_frequencies = calculate_expected_frequencies(observed_data)\n",
        "\n",
        "print(\"Observed Frequencies:\")\n",
        "print(observed_data)\n",
        "\n",
        "print(\"\\nExpected Frequencies:\")\n",
        "print(expected_frequencies)"
      ],
      "metadata": {
        "id": "tviG7xWHzW6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 14. Perform a goodness-of-fit test using Python to compare the observed data to an expected distribution."
      ],
      "metadata": {
        "id": "IAqY91XnADHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import chi2\n",
        "\n",
        "def goodness_of_fit_test(observed, expected):\n",
        "    chi_square_stat = np.sum((observed - expected) ** 2 / expected)\n",
        "\n",
        "    df = len(observed) - 1\n",
        "\n",
        "    p_value = 1 - chi2.cdf(chi_square_stat, df)\n",
        "\n",
        "    return chi_square_stat, p_value\n",
        "\n",
        "observed = np.array([10, 12, 8, 11, 9, 10])\n",
        "\n",
        "expected = np.array([10, 10, 10, 10, 10, 10])\n",
        "\n",
        "chi_square_stat, p_value = goodness_of_fit_test(observed, expected)\n",
        "\n",
        "print(f\"Chi-Square Statistic: {chi_square_stat:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis: The data does not follow the expected distribution.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The data follows the expected distribution.\")"
      ],
      "metadata": {
        "id": "R_x0056H0KLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 15. Create a Python script to simulate and visualize the Chi-square distribution and discuss its characteristics.\n",
        "**Ans** - **Key Characteristics of the Chi-square Distribution**:\n",
        "1. Shape: The Chi-square distribution is asymmetric, with a shape that depends on the degrees of freedom (df). As the degrees of freedom increase, the distribution approaches a normal distribution.\n",
        "2. Skewness: For low degrees of freedom, the distribution is skewed to the right. As the degrees of freedom increase, the skewness decreases.\n",
        "3. Mean and Variance:\n",
        "  * Mean: μ=df\n",
        "  * Variance: σ^2 = 2*df\n",
        "4. Non-Negative: The Chi-square distribution only takes non-negative values (i.e., values greater than or equal to 0)."
      ],
      "metadata": {
        "id": "lygKAmK7AEkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def simulate_chi_square(df, size=1000):\n",
        "    \"\"\"\n",
        "    Simulate data from a Chi-square distribution.\n",
        "\n",
        "    Parameters:\n",
        "    - df: degrees of freedom\n",
        "    - size: number of random samples to generate\n",
        "\n",
        "    Returns:\n",
        "    - An array of simulated Chi-square distributed values.\n",
        "    \"\"\"\n",
        "    return np.random.chisquare(df, size)\n",
        "\n",
        "def plot_chi_square_distribution(df, size=1000):\n",
        "    \"\"\"\n",
        "    Plot the Chi-square distribution for a given degrees of freedom.\n",
        "\n",
        "    Parameters:\n",
        "    - df: degrees of freedom\n",
        "    - size: number of random samples to generate for plotting\n",
        "    \"\"\"\n",
        "\n",
        "    data = simulate_chi_square(df, size)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.hist(data, bins=30, density=True, alpha=0.6, color='b', label=f'Chi-square df={df}')\n",
        "\n",
        "    from scipy.stats import chi2\n",
        "    x = np.linspace(0, np.max(data), 1000)\n",
        "    plt.plot(x, chi2.pdf(x, df), 'r-', lw=2, label='Theoretical Chi-square Distribution')\n",
        "\n",
        "    plt.title(f\"Chi-square Distribution (df = {df})\", fontsize=14)\n",
        "    plt.xlabel('Value', fontsize=12)\n",
        "    plt.ylabel('Density', fontsize=12)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "plot_chi_square_distribution(df=5, size=1000)\n",
        "plot_chi_square_distribution(df=10, size=1000)\n",
        "plot_chi_square_distribution(df=20, size=1000)"
      ],
      "metadata": {
        "id": "fhVJYfLQ05R6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 16. Implement an F-test using Python to compare the variances of two random samples.\n"
      ],
      "metadata": {
        "id": "5UNUHXOfAF8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import f\n",
        "\n",
        "def f_test(sample1, sample2):\n",
        "\n",
        "    var1 = np.var(sample1, ddof=1)\n",
        "    var2 = np.var(sample2, ddof=1)\n",
        "\n",
        "    f_statistic = var1 / var2 if var1 > var2 else var2 / var1\n",
        "\n",
        "    df1 = len(sample1) - 1\n",
        "    df2 = len(sample2) - 1\n",
        "\n",
        "    p_value = 2 * min(f.cdf(f_statistic, df1, df2), 1 - f.cdf(f_statistic, df1, df2))\n",
        "\n",
        "    return f_statistic, p_value\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "sample1 = np.random.normal(loc=50, scale=10, size=100)\n",
        "sample2 = np.random.normal(loc=50, scale=5, size=100)\n",
        "\n",
        "f_statistic, p_value = f_test(sample1, sample2)\n",
        "\n",
        "print(f\"F-statistic: {f_statistic:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis: The variances are significantly different.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The variances are not significantly different.\")"
      ],
      "metadata": {
        "id": "J2jR6tRL3JXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 17. Write a Python program to perform an ANOVA test to compare means between multiple groups and interpret the results."
      ],
      "metadata": {
        "id": "aZpdAseGAOl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "group1 = np.random.normal(loc=50, scale=10, size=30)\n",
        "\n",
        "group2 = np.random.normal(loc=55, scale=10, size=30)\n",
        "\n",
        "group3 = np.random.normal(loc=60, scale=10, size=30)\n",
        "\n",
        "f_statistic, p_value = f_oneway(group1, group2, group3)\n",
        "\n",
        "print(f\"F-statistic: {f_statistic:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis: At least one group mean is significantly different.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The group means are not significantly different.\")"
      ],
      "metadata": {
        "id": "4avWSQCK322Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 18. Perform a one-way ANOVA test using Python to compare the means of different groups and plot the results."
      ],
      "metadata": {
        "id": "sP256Uj-AQcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "group1 = np.random.normal(loc=50, scale=10, size=30)\n",
        "group2 = np.random.normal(loc=55, scale=10, size=30)\n",
        "group3 = np.random.normal(loc=60, scale=10, size=30)\n",
        "\n",
        "f_statistic, p_value = f_oneway(group1, group2, group3)\n",
        "\n",
        "print(f\"F-statistic: {f_statistic:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis: At least one group mean is significantly different.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The group means are not significantly different.\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.boxplot([group1, group2, group3], labels=[\"Group 1\", \"Group 2\", \"Group 3\"])\n",
        "\n",
        "plt.title(\"One-Way ANOVA: Comparing Means of Different Groups\", fontsize=14)\n",
        "plt.xlabel(\"Groups\", fontsize=12)\n",
        "plt.ylabel(\"Values\", fontsize=12)\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eCvqMnuY41Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 19. Write a Python function to check the assumptions (normality, independence, and equal variance) for ANOVA."
      ],
      "metadata": {
        "id": "jNsl8wdUASBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def check_anova_assumptions(groups):\n",
        "    normality_results = {}\n",
        "    for i, group in enumerate(groups, 1):\n",
        "        stat, p_value = stats.shapiro(group)\n",
        "        normality_results[f'Group {i}'] = (stat, p_value)\n",
        "        if p_value < 0.05:\n",
        "            print(f\"Group {i} does not follow a normal distribution (p-value = {p_value:.4f})\")\n",
        "        else:\n",
        "            print(f\"Group {i} follows a normal distribution (p-value = {p_value:.4f})\")\n",
        "\n",
        "        plt.figure()\n",
        "        stats.probplot(group, dist=\"norm\", plot=plt)\n",
        "        plt.title(f\"Q-Q Plot for Group {i}\")\n",
        "        plt.show()\n",
        "\n",
        "    stat, p_value = stats.levene(*groups)\n",
        "    print(f\"\\nLevene's Test for Equal Variance: p-value = {p_value:.4f}\")\n",
        "    if p_value < 0.05:\n",
        "        print(\"The variances are significantly different (violating the equal variance assumption).\")\n",
        "    else:\n",
        "        print(\"The variances are equal (assumption of equal variance holds).\")\n",
        "\n",
        "    print(\"\\nIndependence is assumed by design. Please check your study design for independence.\")\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "group1 = np.random.normal(loc=50, scale=10, size=30)\n",
        "group2 = np.random.normal(loc=55, scale=10, size=30)\n",
        "group3 = np.random.normal(loc=60, scale=10, size=30)\n",
        "\n",
        "groups = [group1, group2, group3]\n",
        "\n",
        "check_anova_assumptions(groups)"
      ],
      "metadata": {
        "id": "1Kt-zMy-5jXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 20. Perform a two-way ANOVA test using Python to study the interaction between two factors and visualize the results."
      ],
      "metadata": {
        "id": "1N0U5mnBAUBJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans** - A two-way ANOVA test is used to study the effect of two independent variables (factors) on a dependent variable, and it can also assess if there is an interaction effect between the two factors. In a two-way ANOVA, there are three main hypotheses:\n",
        "1. Main Effect of Factor 1: Whether the levels of the first factor have a significant effect on the dependent variable.\n",
        "2. Main Effect of Factor 2: Whether the levels of the second factor have a significant effect on the dependent variable.\n",
        "3. Interaction Effect: Whether there is an interaction between the two factors, i.e., whether the effect of one factor depends on the level of the other factor."
      ],
      "metadata": {
        "id": "X8OmxIG8z8D_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "from scipy.stats import f\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "treatment = ['A', 'B', 'C']\n",
        "\n",
        "time = ['Morning', 'Evening']\n",
        "\n",
        "data = []\n",
        "for t in treatment:\n",
        "    for tm in time:\n",
        "        for i in range(30):\n",
        "            value = np.random.normal(loc=50, scale=10) + (ord(t) - ord('A')) * 5 + (ord(tm[0]) - ord('M')) * 3\n",
        "            data.append([t, tm, value])\n",
        "\n",
        "df = pd.DataFrame(data, columns=['Treatment', 'Time', 'Score'])\n",
        "\n",
        "model = ols('Score ~ C(Treatment) + C(Time) + C(Treatment):C(Time)', data=df).fit()\n",
        "anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "\n",
        "print(anova_table)\n",
        "\n",
        "if anova_table['PR(>F)'][0] < 0.05:\n",
        "    print(\"Main effect of Treatment is significant.\")\n",
        "else:\n",
        "    print(\"Main effect of Treatment is not significant.\")\n",
        "\n",
        "if anova_table['PR(>F)'][1] < 0.05:\n",
        "    print(\"Main effect of Time is significant.\")\n",
        "else:\n",
        "    print(\"Main effect of Time is not significant.\")\n",
        "\n",
        "if anova_table['PR(>F)'][2] < 0.05:\n",
        "    print(\"Interaction effect between Treatment and Time is significant.\")\n",
        "else:\n",
        "    print(\"Interaction effect between Treatment and Time is not significant.\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x='Treatment', y='Score', hue='Time', data=df, palette=\"Set2\")\n",
        "plt.title(\"Two-Way ANOVA: Interaction between Treatment and Time\", fontsize=14)\n",
        "plt.xlabel(\"Treatment\", fontsize=12)\n",
        "plt.ylabel(\"Score\", fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MqzhjY6F6QN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 21. Write a Python program to visualize the F-distribution and discuss its use in hypothesis testing."
      ],
      "metadata": {
        "id": "Vna1lXmaAV_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import f\n",
        "\n",
        "df1_values = [1, 5, 10]\n",
        "df2_values = [10, 20, 30]\n",
        "\n",
        "x = np.linspace(0, 5, 1000)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for df1 in df1_values:\n",
        "    for df2 in df2_values:\n",
        "        y = f.pdf(x, df1, df2)\n",
        "        plt.plot(x, y, label=f'df1={df1}, df2={df2}')\n",
        "\n",
        "plt.title('F-Distribution for Different Degrees of Freedom', fontsize=14)\n",
        "plt.xlabel('F Value', fontsize=12)\n",
        "plt.ylabel('Probability Density', fontsize=12)\n",
        "plt.legend(title='Degrees of Freedom')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sKftow547Lu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use of F-Distribution in Hypothesis Testing**:\n",
        "\n",
        "The F-distribution is most commonly used in hypothesis testing for:\n",
        "1. Comparing variances: For example, the F-test is used to compare the variances of two populations.\n",
        "* Null hypothesis (H0): The variances of the two populations are equal.\n",
        "* Alternative hypothesis (H1): The variances of the two populations are not equal.\n",
        "* The F-statistic is the ratio of the two sample variances, and if it is large, it suggests a significant difference between the variances.\n",
        "2. ANOVA (Analysis of Variance):\n",
        "* In one-way or two-way ANOVA, the F-statistic tests whether the group means are significantly different. The F-statistic is calculated as the ratio of the variance between groups to the variance within groups.\n",
        "* If the F-statistic is large (i.e., significantly greater than 1), it indicates that the means of the groups are likely different.\n",
        "\n",
        "The F-statistic for ANOVA is given by:\n",
        "\n",
        "    F = Variance Between Groups/Variance Within Groups\n",
        "3. Regression Analysis:\n",
        "* The F-statistic is used to test the overall significance of the regression model. It compares the model with no predictors (intercept-only model) to the model with predictors."
      ],
      "metadata": {
        "id": "0ojma7sH7koL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 22. Perform a one-way ANOVA test in Python and visualize the results with boxplots to compare group means."
      ],
      "metadata": {
        "id": "RDEKc4DaAXWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "group1 = np.random.normal(loc=50, scale=10, size=30)\n",
        "group2 = np.random.normal(loc=55, scale=10, size=30)\n",
        "group3 = np.random.normal(loc=60, scale=10, size=30)\n",
        "\n",
        "f_stat, p_value = stats.f_oneway(group1, group2, group3)\n",
        "\n",
        "print(f\"F-statistic: {f_stat:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"There is a significant difference between the group means.\")\n",
        "else:\n",
        "    print(\"There is no significant difference between the group means.\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(data=[group1, group2, group3], palette=\"Set2\")\n",
        "plt.xticks([0, 1, 2], ['Group 1', 'Group 2', 'Group 3'])\n",
        "plt.title('One-Way ANOVA: Comparison of Group Means', fontsize=14)\n",
        "plt.xlabel('Group', fontsize=12)\n",
        "plt.ylabel('Score', fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yhP1cSs29GQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualization:**\n",
        "\n",
        "The boxplot will show the distribution of scores for each group, allowing you to visually assess the differences in group means. If the groups are significantly different, the boxplots will show a clear separation between the group medians.\n",
        "\n",
        "**Interpretation:**\n",
        "* If the p-value is less than 0.05, we reject the null hypothesis, meaning there is a significant difference between the means of at least two groups.\n",
        "* The F-statistic is a measure of the ratio of between-group variance to within-group variance. A large F-statistic indicates that the group means are likely different from each other."
      ],
      "metadata": {
        "id": "REs2oEn9y-9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 23. Simulate random data from a normal distribution, then perform hypothesis testing to evaluate the means."
      ],
      "metadata": {
        "id": "hSlwQ8E5AZJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "group1 = np.random.normal(loc=50, scale=10, size=100)\n",
        "group2 = np.random.normal(loc=55, scale=10, size=100)\n",
        "\n",
        "t_stat, p_value = stats.ttest_ind(group1, group2)\n",
        "\n",
        "print(f\"T-statistic: {t_stat:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis: There is a significant difference between the group means.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: There is no significant difference between the group means.\")"
      ],
      "metadata": {
        "id": "xoGjC5Sh-h05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 24. Perform a hypothesis test for population variance using a Chi-square distribution and interpret the results."
      ],
      "metadata": {
        "id": "hEWlzaWUAaoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "np.random.seed(42)\n",
        "n = 30\n",
        "sigma_squared = 100\n",
        "sample = np.random.normal(loc=50, scale=10, size=n)\n",
        "sample_variance = np.var(sample, ddof=1)\n",
        "chi_square_statistic = (n - 1) * sample_variance / sigma_squared\n",
        "\n",
        "df = n - 1\n",
        "p_value = 2 * min(stats.chi2.cdf(chi_square_statistic, df), 1 - stats.chi2.cdf(chi_square_statistic, df))\n",
        "print(f\"Chi-square Statistic: {chi_square_statistic:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: The population variance is significantly different from the hypothesized value.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The population variance is not significantly different from the hypothesized value.\")"
      ],
      "metadata": {
        "id": "xBC1RRvT_gFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation**:\n",
        "* The Chi-square statistic of 34.3980 is based on the sample data.\n",
        "* The p-value is very small (0.0012), which is less than the significance level of 0.05.\n",
        "* Thus, we reject the null hypothesis and conclude that the population variance is significantly different from the hypothesized value of 100."
      ],
      "metadata": {
        "id": "8TVBDyU7_mkn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 25. Write a Python script to perform a Z-test for comparing proportions between two datasets or groups."
      ],
      "metadata": {
        "id": "MSC2_-9tAcNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "x1 = 60\n",
        "n1 = 100\n",
        "x2 = 50\n",
        "n2 = 100\n",
        "p1 = x1 / n1\n",
        "p2 = x2 / n2\n",
        "P = (x1 + x2) / (n1 + n2)\n",
        "Z = (p1 - p2) / np.sqrt(P * (1 - P) * (1/n1 + 1/n2))\n",
        "p_value = 2 * (1 - stats.norm.cdf(abs(Z)))\n",
        "\n",
        "print(f\"Z-statistic: {Z:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: There is a significant difference between the proportions.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: There is no significant difference between the proportions.\")"
      ],
      "metadata": {
        "id": "LYMwlU6DEUu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 26. Implement an F-test for comparing the variances of two datasets, then interpret and visualize the results."
      ],
      "metadata": {
        "id": "8Kh0GNs7AeHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n",
        "data1 = np.random.normal(loc=50, scale=10, size=100)\n",
        "data2 = np.random.normal(loc=50, scale=20, size=100)\n",
        "\n",
        "var1 = np.var(data1, ddof=1)\n",
        "var2 = np.var(data2, ddof=1)\n",
        "F_statistic = var1 / var2 if var1 > var2 else var2 / var1\n",
        "\n",
        "df1 = len(data1) - 1\n",
        "df2 = len(data2) - 1\n",
        "p_value = 1 - stats.f.cdf(F_statistic, df1, df2)\n",
        "print(f\"F-statistic: {F_statistic:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: The variances are significantly different.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The variances are not significantly different.\")\n",
        "x = np.linspace(0, 5, 1000)\n",
        "y = stats.f.pdf(x, df1, df2)\n",
        "plt.plot(x, y, label=\"F-distribution\")\n",
        "plt.axvline(F_statistic, color='r', linestyle='--', label=f\"F-statistic = {F_statistic:.2f}\")\n",
        "plt.title('F-distribution with F-statistic')\n",
        "plt.xlabel('F value')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hcn8WjSDGR9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation of Results:**\n",
        "* The F-statistic of 0.2500 suggests that the ratio of the sample variances is quite small.\n",
        "* The p-value is very high (0.9999), which is much greater than the significance level of 0.05, so we fail to reject the null hypothesis. This suggests that there is no significant difference between the variances of the two datasets.\n",
        "\n",
        "**Visualization:**\n",
        "* The F-distribution is shown, and the calculated F-statistic is marked with a red dashed line.\n",
        "* Since the F-statistic falls within the region where we would expect values if the null hypothesis is true, this further confirms that we fail to reject the null hypothesis."
      ],
      "metadata": {
        "id": "_mxQhhaEGQXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 27. Perform a Chi-square test for goodness of fit with simulated data and analyze the results."
      ],
      "metadata": {
        "id": "Fsl2jKchDEMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simulating data from a fair die roll and testing if it follows a uniform distribution**\n",
        "\n",
        "We'll simulate 100 rolls of a fair die and test if the outcomes are uniformly distributed, meaning each face of the die (1, 2, 3, 4, 5, and 6) should appear approximately the same number of times."
      ],
      "metadata": {
        "id": "fxfM41996zzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n",
        "observed_data = np.random.randint(1, 7, size=100)\n",
        "observed_frequencies = [np.sum(observed_data == i) for i in range(1, 7)]\n",
        "\n",
        "expected_frequencies = [100 / 6] * 6\n",
        "chi2_statistic, p_value = stats.chisquare(observed_frequencies, expected_frequencies)\n",
        "print(f\"Chi-square Statistic: {chi2_statistic:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: The data does not follow a uniform distribution.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The data follows a uniform distribution.\")\n",
        "\n",
        "x = range(1, 7)\n",
        "plt.bar(x, observed_frequencies, label=\"Observed\", alpha=0.6, color='blue')\n",
        "plt.plot(x, expected_frequencies, label=\"Expected\", color='red', marker='o', linestyle='--', linewidth=2)\n",
        "plt.title('Chi-square Goodness of Fit Test for Die Rolls')\n",
        "plt.xlabel('Die Face')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LXAkJE7zH6vU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:**\n",
        "* The Chi-square statistic is 5.6000, which represents the difference between the observed and expected frequencies.\n",
        "* The p-value is 0.4702, which is much greater than the significance level of 0.05.\n",
        "* Since the p-value is greater than 0.05, we fail to reject the null hypothesis, meaning there is no significant difference between the observed and expected frequencies. Thus, we conclude that the die rolls are consistent with a uniform distribution."
      ],
      "metadata": {
        "id": "HDeiNDwBH8K6"
      }
    }
  ]
}